{"cells":[{"cell_type":"markdown","source":["# 1. 개발 환경 소개"],"metadata":{"id":"QbTe8W3EGD3x"}},{"cell_type":"markdown","source":["- Google Colab GPU 환경에서 진행했습니다.\n","- 개발 환경은 다음과 같습니다."],"metadata":{"id":"LzLOGZf6zbLk"}},{"cell_type":"code","source":["import platform\n","print('- os:',platform.platform())\n","print('- 운영체제:', end=\"\")\n","!cat /etc/issue.net\n","print('- Process information:', platform.processor())\n","print('- Process Architecture:', platform.machine())\n","print(\"- RAM: 12.68GB\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KD9IbQrWzoX8","executionInfo":{"status":"ok","timestamp":1660300367077,"user_tz":-540,"elapsed":8,"user":{"displayName":"안도현","userId":"08395434251954901727"}},"outputId":"ede2e41c-8097-4c5a-fd68-68a7bcba9eff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["- os: Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\n","- 운영체제:Ubuntu 18.04.6 LTS\n","- Process information: x86_64\n","- Process Architecture: x86_64\n","- RAM: 12.68GB\n"]}]},{"cell_type":"code","source":["print('GPU')\n","!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XT_BZOkEaau6","executionInfo":{"status":"ok","timestamp":1660300369599,"user_tz":-540,"elapsed":7,"user":{"displayName":"안도현","userId":"08395434251954901727"}},"outputId":"857b3b66-61e0-45ef-c199-c48c27db48e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU\n","Fri Aug 12 10:32:48 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   63C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","source":["# 2. 라이브러리 불러오기 및 경로 지정"],"metadata":{"id":"w-mkqlXQB2I_"}},{"cell_type":"markdown","source":["### 라이브러리 불러오기"],"metadata":{"id":"dz7OBfGiB7k2"}},{"cell_type":"code","source":["# 전처리 라이브러리\n","import pandas as pd\n","import numpy as np\n","import os\n","import ast\n","from datetime import datetime, timedelta\n","import datetime\n","from tqdm import tqdm\n","import random\n","\n","# 분석 라이브러리\n","import torch\n","import tensorflow as tf\n","from keras.models import load_model\n","import torch\n","\n","# 분석에 문제가 없는 경고 메시지 숨김\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"V5H19tojePzf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 라이브러리 버전"],"metadata":{"id":"9xGwsXZ0vNHE"}},{"cell_type":"code","source":["print('- ', end=\"\")\n","!python --version\n","print('- pandas:', pd.__version__)\n","print('- numpy:', np.__version__)\n","print('- torch:', torch.__version__)\n","print('- tensorflow:', tf.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UCCNilU_A9J0","executionInfo":{"status":"ok","timestamp":1660300378089,"user_tz":-540,"elapsed":12,"user":{"displayName":"안도현","userId":"08395434251954901727"}},"outputId":"2ea7ccae-af72-4373-ce95-bdc49c32cfc4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["- Python 3.7.13\n","- pandas: 1.3.5\n","- numpy: 1.21.6\n","- torch: 1.12.0+cu113\n","- tensorflow: 2.8.2\n"]}]},{"cell_type":"markdown","source":["### 경로 설정 및 데이터 불러오기"],"metadata":{"id":"PoyQkZ1-CUYH"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fvh5iFzelb1O","executionInfo":{"status":"ok","timestamp":1660300441182,"user_tz":-540,"elapsed":63098,"user":{"displayName":"안도현","userId":"08395434251954901727"}},"outputId":"c98e24e1-6382-4211-8565-fec21af8655e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# 구글 드라이브 연결\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["코드 실행을 위해 데이터가 있는 폴더를 설정합니다."],"metadata":{"id":"xKuX41e0iKON"}},{"cell_type":"code","source":["data_dir = '/content/drive/MyDrive/롯데멤버스_경진대회/3. 안다비젼_ 데이터 및 모델 세이브 파일/'"],"metadata":{"id":"vtzgPn33P-Mx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 불러오기\n","data_train = pd.read_csv(data_dir+'output/model_data_train.csv') # 전체 주문건 - 훈련용\n","data_test = pd.read_csv(data_dir+'output/model_data_test.csv') # 전체 주문건 - 테스트용\n","user_data_train = pd.read_csv(data_dir+'output/user_data_train.csv') # 유저별 주문정보 - 훈련용\n","user_data_test = pd.read_csv(data_dir+'output/user_data_test.csv') # 유저별 주문정보 - 테스트용\n","pd_clac = pd.read_csv(data_dir+'data/LPOINT_BIG_COMP_04_PD_CLAC.csv') # 상품 분류 정보 - 유통사 상품 카테고리 마스터 "],"metadata":{"id":"rxdWxkeBeXhG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. 후보 모델"],"metadata":{"id":"6jCcSHGSJ2_D"}},{"cell_type":"markdown","source":["* 저장해둔 데이터에서 문자열을 리스트로 변환합니다."],"metadata":{"id":"PfW8SUqwboDA"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36379,"status":"ok","timestamp":1660300486708,"user":{"displayName":"안도현","userId":"08395434251954901727"},"user_tz":-540},"id":"q4FTTeteqydR","outputId":"f81ac12f-1d6d-46ab-ce74-7b9c62e24ce2"},"outputs":[{"output_type":"stream","name":"stdout","text":["1. 숫자 -> 리스트 변환\n","2. 문자열 -> 리스트 변환\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5/5 [00:27<00:00,  5.54s/it]\n","100%|██████████| 8751/8751 [00:04<00:00, 1783.73it/s]\n","100%|██████████| 4074/4074 [00:01<00:00, 2146.71it/s]\n"]}],"source":["# 1. 데이터 복사\n","data_train_tmp = data_train.copy()\n","data_test_tmp = data_test.copy() # 이후에 중복제거용으로 사용\n","\n","# 2. 리스트 변환\n","print(\"1. 숫자 -> 리스트 변환\")\n","for i in ['gender', 'region', 'ages']:\n","  data_train[i] = [[x] for x in list(data_train[i])]\n","  data_test[i] = [[x] for x in list(data_test[i])]  \n","\n","print(\"2. 문자열 -> 리스트 변환\")\n","for i in tqdm(['order', 'product', 'order_dow', 'order_hour_of_day', 'day_since_prior_order']):\n","  for j in user_data_train.index:\n","    try: user_data_train[i][j] = list(map(int, ast.literal_eval(user_data_train[i][j])))\n","    except: pass\n","  for j in user_data_test.index:\n","    try: user_data_test[i][j] = list(map(int, ast.literal_eval(user_data_test[i][j])))  \n","    except: pass\n","\n","for j in tqdm(user_data_train.index):\n","    try: user_data_train['buy_am'][j] = list(map(float, ast.literal_eval(user_data_train['buy_am'][j])))\n","    except: pass\n","for j in tqdm(user_data_test.index):\n","    try: user_data_test['buy_am'][j] = list(map(float, ast.literal_eval(user_data_test['buy_am'][j])))\n","    except: pass"]},{"cell_type":"markdown","source":["* 후보 모델에 필요한 파라미터와 커스텀 레이어를 정의합니다."],"metadata":{"id":"07NWYmBYbrm4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3LHliVBYAdov"},"outputs":[],"source":["# 3. 하이퍼파라미터 정의\n","EMBEDDING_DIMS = 64\n","DENSE_UNITS = 2048 # dense layer 뉴런 개수\n","DROPOUT_PCT = 0.1 # dropout\n","ALPHA = 0.1\n","NUM_CLASSES = data_train[\"product\"].max() + 1 # 큰거 기준\n","LEARNING_RATE = 0.01\n","\n","# 4. custom layers 정의\n","class MaskedEmbeddingsAggregatorLayer(tf.keras.layers.Layer):\n","    def __init__(self, agg_mode='sum', **kwargs):\n","        super(MaskedEmbeddingsAggregatorLayer, self).__init__(**kwargs)\n","\n","        if agg_mode not in ['sum', 'mean']:\n","            raise NotImplementedError('mode {} not implemented!'.format(agg_mode))\n","        self.agg_mode = agg_mode\n","    \n","    @tf.function\n","    def call(self, inputs, mask=None):\n","        masked_embeddings = tf.ragged.boolean_mask(inputs, mask)\n","        if self.agg_mode == 'sum':\n","            aggregated =  tf.reduce_sum(masked_embeddings, axis=1)\n","        elif self.agg_mode == 'mean':\n","            aggregated = tf.reduce_mean(masked_embeddings, axis=1)\n","        return aggregated\n","    \n","    def get_config(self):\n","        return {'agg_mode': self.agg_mode}\n","    \n","class L2NormLayer(tf.keras.layers.Layer):\n","    def __init__(self, **kwargs):\n","        super(L2NormLayer, self).__init__(**kwargs)\n","    \n","    @tf.function\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            inputs = tf.ragged.boolean_mask(inputs, mask).to_tensor()\n","        return tf.math.l2_normalize(inputs, axis=-1)\n","\n","    def compute_mask(self, inputs, mask):\n","        return mask"]},{"cell_type":"markdown","source":["* 후보 모델을 정의합니다"],"metadata":{"id":"IW5aNpt0bxJC"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7812,"status":"ok","timestamp":1660300494516,"user":{"displayName":"안도현","userId":"08395434251954901727"},"user_tz":-540},"id":"8iv1Y7aoAw3h","outputId":"cb04c034-f912-4cc2-8dce-0f378c00035c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," product_hist (InputLayer)      [(None, None)]       0           []                               \n","                                                                                                  \n"," order_dow_hist (InputLayer)    [(None, None)]       0           []                               \n","                                                                                                  \n"," order_hour_of_day_hist (InputL  [(None, None)]      0           []                               \n"," ayer)                                                                                            \n","                                                                                                  \n"," days_since_prior_order_hist (I  [(None, None)]      0           []                               \n"," nputLayer)                                                                                       \n","                                                                                                  \n"," buy_am_hist (InputLayer)       [(None, None)]       0           []                               \n","                                                                                                  \n"," labels_embeddings (Embedding)  (None, None, 64)     89664       ['product_hist[0][0]',           \n","                                                                  'order_dow_hist[0][0]',         \n","                                                                  'order_hour_of_day_hist[0][0]', \n","                                                                  'days_since_prior_order_hist[0][\n","                                                                 0]',                             \n","                                                                  'buy_am_hist[0][0]']            \n","                                                                                                  \n"," l2_norm_1 (L2NormLayer)        (None, None, 64)     0           ['labels_embeddings[0][0]',      \n","                                                                  'labels_embeddings[1][0]',      \n","                                                                  'labels_embeddings[2][0]',      \n","                                                                  'labels_embeddings[3][0]',      \n","                                                                  'labels_embeddings[4][0]']      \n","                                                                                                  \n"," aggregate_embeddings (MaskedEm  (None, 64)          0           ['l2_norm_1[1][0]',              \n"," beddingsAggregatorLayer)                                         'l2_norm_1[2][0]',              \n","                                                                  'l2_norm_1[3][0]',              \n","                                                                  'l2_norm_1[4][0]',              \n","                                                                  'l2_norm_1[5][0]']              \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 320)          0           ['aggregate_embeddings[1][0]',   \n","                                                                  'aggregate_embeddings[2][0]',   \n","                                                                  'aggregate_embeddings[3][0]',   \n","                                                                  'aggregate_embeddings[4][0]',   \n","                                                                  'aggregate_embeddings[5][0]']   \n","                                                                                                  \n"," dense_1 (Dense)                (None, 2048)         657408      ['concatenate[0][0]']            \n","                                                                                                  \n"," dense_1_relu (ReLU)            (None, 2048)         0           ['dense_1[0][0]']                \n","                                                                                                  \n"," dense_2 (Dense)                (None, 2048)         4196352     ['dense_1_relu[0][0]']           \n","                                                                                                  \n"," dense_2_relu (ReLU)            (None, 2048)         0           ['dense_2[0][0]']                \n","                                                                                                  \n"," dense_3 (Dense)                (None, 2048)         4196352     ['dense_2_relu[0][0]']           \n","                                                                                                  \n"," dense_3_relu (ReLU)            (None, 2048)         0           ['dense_3[0][0]']                \n","                                                                                                  \n"," dense_3_batch_norm (BatchNorma  (None, 2048)        8192        ['dense_3_relu[0][0]']           \n"," lization)                                                                                        \n","                                                                                                  \n"," dense_output (Dense)           (None, 1401)         2870649     ['dense_3_batch_norm[0][0]']     \n","                                                                                                  \n","==================================================================================================\n","Total params: 12,018,617\n","Trainable params: 12,014,521\n","Non-trainable params: 4,096\n","__________________________________________________________________________________________________\n"]}],"source":["# 5. modeling\n","input_user = tf.keras.Input(shape=(None, ), name='user')\n","input_product_hist = tf.keras.layers.Input(shape=(None,), name='product_hist')\n","input_order_dow_hist = tf.keras.layers.Input(shape=(None,), name='order_dow_hist')\n","input_order_hour_of_day_hist = tf.keras.Input(shape=(None, ), name='order_hour_of_day_hist')\n","input_days_since_prior_order_hist = tf.keras.Input(shape=(None, ), name='days_since_prior_order_hist')\n","input_buy_am_hist = tf.keras.Input(shape=(None, ), name='buy_am_hist')\n","\n","\n","# 5-1. layer 구성\n","features_embedding_layer = tf.keras.layers.Embedding(input_dim=NUM_CLASSES, output_dim=EMBEDDING_DIMS,  mask_zero=True, trainable=True, name='features_embeddings')\n","labels_embedding_layer = tf.keras.layers.Embedding(input_dim=NUM_CLASSES, output_dim=EMBEDDING_DIMS,  mask_zero=True, trainable=True, name='labels_embeddings')\n","avg_embeddings = MaskedEmbeddingsAggregatorLayer(agg_mode='mean', name='aggregate_embeddings')\n","\n","dense_1 = tf.keras.layers.Dense(units=DENSE_UNITS, name='dense_1')\n","dense_2 = tf.keras.layers.Dense(units=DENSE_UNITS, name='dense_2')\n","dense_3 = tf.keras.layers.Dense(units=DENSE_UNITS, name='dense_3')\n","l2_norm_1 = L2NormLayer(name='l2_norm_1')\n","dense_output = tf.keras.layers.Dense(NUM_CLASSES, activation=tf.nn.softmax, name='dense_output')\n","\n","# 5-2. feature 투입\n","features_embeddings = features_embedding_layer(input_user)\n","l2_norm_features = l2_norm_1(features_embeddings)\n","avg_features = avg_embeddings(l2_norm_features)\n","\n","labels_product_embeddings = labels_embedding_layer(input_product_hist)\n","l2_norm_product = l2_norm_1(labels_product_embeddings)\n","avg_product = avg_embeddings(l2_norm_product)\n","\n","labels_order_dow_embeddings = labels_embedding_layer(input_order_dow_hist)\n","l2_norm_order_dow = l2_norm_1(labels_order_dow_embeddings)\n","avg_order_dow = avg_embeddings(l2_norm_order_dow)\n","\n","labels_order_hour_embeddings = labels_embedding_layer(input_order_hour_of_day_hist)\n","l2_norm_order_hour = l2_norm_1(labels_order_hour_embeddings)\n","avg_order_hour = avg_embeddings(l2_norm_order_hour)\n","\n","labels_since_prior_embeddings = labels_embedding_layer(input_days_since_prior_order_hist)\n","l2_norm_since_prior = l2_norm_1(labels_since_prior_embeddings)\n","avg_since_prior = avg_embeddings(l2_norm_since_prior)\n","\n","labels_buy_am_embeddings = labels_embedding_layer(input_buy_am_hist)\n","l2_norm_buy_prior = l2_norm_1(labels_buy_am_embeddings)\n","avg_buy_prior = avg_embeddings(l2_norm_buy_prior)\n","\n","# 5-3. 임베딩 벡터들 연결\n","concat_inputs = tf.keras.layers.Concatenate(axis=1)([avg_product, avg_order_dow, avg_order_hour, avg_since_prior, avg_buy_prior ])\n","\n","# 5-4. Dense Layers 구성\n","dense_1_features = dense_1(concat_inputs)\n","dense_1_relu = tf.keras.layers.ReLU(name='dense_1_relu')(dense_1_features)\n","dense_1_batch_norm = tf.keras.layers.BatchNormalization(name='dense_1_batch_norm')(dense_1_relu)\n","\n","dense_2_features = dense_2(dense_1_relu)\n","dense_2_relu = tf.keras.layers.ReLU(name='dense_2_relu')(dense_2_features)\n","dense_2_batch_norm = tf.keras.layers.BatchNormalization(name='dense_2_batch_norm')(dense_2_relu)\n","\n","dense_3_features = dense_3(dense_2_relu)\n","dense_3_relu = tf.keras.layers.ReLU(name='dense_3_relu')(dense_3_features)\n","dense_3_batch_norm = tf.keras.layers.BatchNormalization(name='dense_3_batch_norm')(dense_3_relu)\n","\n","outputs = dense_output(dense_3_batch_norm)\n","\n","# 5-5. Optimizer 정의\n","optimiser = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n","\n","# 5-6. 모델 준비 및 컴파일\n","model = tf.keras.models.Model(\n","    inputs=[input_product_hist,\n","            input_order_dow_hist,\n","            input_order_hour_of_day_hist,\n","            input_days_since_prior_order_hist,\n","            input_buy_am_hist,\n","            ],\n","    outputs=[outputs]\n",")\n","model.compile(optimizer=optimiser, loss='sparse_categorical_crossentropy', metrics=['acc']) \n","model.summary()"]},{"cell_type":"markdown","source":["* 입력 데이터 개수를 정의하고 훈련을 진행합니다."],"metadata":{"id":"PbI1SyJMbzzS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZKxxLz0sP6ax","executionInfo":{"status":"ok","timestamp":1660301658139,"user_tz":-540,"elapsed":1163640,"user":{"displayName":"안도현","userId":"08395434251954901727"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"888f7dba-148d-4ae7-8b98-fa76ed9a5150"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/600\n","63/63 [==============================] - 8s 30ms/step - loss: 11.5256 - acc: 0.0000e+00\n","Epoch 2/600\n","63/63 [==============================] - 2s 29ms/step - loss: 8.1037 - acc: 4.9975e-04\n","Epoch 3/600\n","63/63 [==============================] - 2s 29ms/step - loss: 7.2085 - acc: 9.9950e-04\n","Epoch 4/600\n","63/63 [==============================] - 2s 30ms/step - loss: 6.9552 - acc: 0.0015\n","Epoch 5/600\n","63/63 [==============================] - 2s 29ms/step - loss: 6.8813 - acc: 0.0030\n","Epoch 6/600\n","63/63 [==============================] - 2s 28ms/step - loss: 6.8330 - acc: 0.0035\n","Epoch 7/600\n","63/63 [==============================] - 2s 29ms/step - loss: 6.7487 - acc: 0.0035\n","Epoch 8/600\n","63/63 [==============================] - 2s 29ms/step - loss: 6.6932 - acc: 0.0040\n","Epoch 9/600\n","63/63 [==============================] - 2s 29ms/step - loss: 6.6371 - acc: 0.0040\n","Epoch 10/600\n","63/63 [==============================] - 2s 29ms/step - loss: 6.6075 - acc: 0.0030\n","Epoch 11/600\n","63/63 [==============================] - 2s 29ms/step - loss: 6.5525 - acc: 9.9950e-04\n","Epoch 12/600\n","63/63 [==============================] - 2s 34ms/step - loss: 6.4996 - acc: 0.0045\n","Epoch 13/600\n","63/63 [==============================] - 2s 38ms/step - loss: 6.4551 - acc: 0.0045\n","Epoch 14/600\n","63/63 [==============================] - 2s 29ms/step - loss: 6.4390 - acc: 0.0040\n","Epoch 15/600\n","63/63 [==============================] - 2s 29ms/step - loss: 6.3000 - acc: 0.0050\n","Epoch 16/600\n","63/63 [==============================] - 2s 29ms/step - loss: 6.2263 - acc: 0.0070\n","Epoch 17/600\n","63/63 [==============================] - 2s 29ms/step - loss: 6.1627 - acc: 0.0055\n","Epoch 18/600\n","63/63 [==============================] - 2s 29ms/step - loss: 6.1198 - acc: 0.0105\n","Epoch 19/600\n","63/63 [==============================] - 2s 29ms/step - loss: 6.0542 - acc: 0.0110\n","Epoch 20/600\n","63/63 [==============================] - 2s 29ms/step - loss: 5.9540 - acc: 0.0100\n","Epoch 21/600\n","63/63 [==============================] - 2s 30ms/step - loss: 5.9266 - acc: 0.0090\n","Epoch 22/600\n","63/63 [==============================] - 2s 29ms/step - loss: 5.8088 - acc: 0.0165\n","Epoch 23/600\n","63/63 [==============================] - 2s 30ms/step - loss: 5.8044 - acc: 0.0125\n","Epoch 24/600\n","63/63 [==============================] - 2s 30ms/step - loss: 5.7212 - acc: 0.0200\n","Epoch 25/600\n","63/63 [==============================] - 2s 30ms/step - loss: 5.6323 - acc: 0.0170\n","Epoch 26/600\n","63/63 [==============================] - 2s 29ms/step - loss: 5.5548 - acc: 0.0235\n","Epoch 27/600\n","63/63 [==============================] - 2s 29ms/step - loss: 5.5514 - acc: 0.0200\n","Epoch 28/600\n","63/63 [==============================] - 2s 29ms/step - loss: 5.5196 - acc: 0.0210\n","Epoch 29/600\n","63/63 [==============================] - 2s 29ms/step - loss: 5.3460 - acc: 0.0345\n","Epoch 30/600\n","63/63 [==============================] - 2s 29ms/step - loss: 5.2980 - acc: 0.0325\n","Epoch 31/600\n","63/63 [==============================] - 2s 29ms/step - loss: 5.1832 - acc: 0.0460\n","Epoch 32/600\n","63/63 [==============================] - 2s 29ms/step - loss: 5.0897 - acc: 0.0490\n","Epoch 33/600\n","63/63 [==============================] - 2s 30ms/step - loss: 5.0855 - acc: 0.0535\n","Epoch 34/600\n","63/63 [==============================] - 2s 29ms/step - loss: 4.8420 - acc: 0.0660\n","Epoch 35/600\n","63/63 [==============================] - 2s 29ms/step - loss: 4.7848 - acc: 0.0800\n","Epoch 36/600\n","63/63 [==============================] - 2s 29ms/step - loss: 4.6637 - acc: 0.0910\n","Epoch 37/600\n","63/63 [==============================] - 2s 29ms/step - loss: 4.5791 - acc: 0.0980\n","Epoch 38/600\n","63/63 [==============================] - 2s 29ms/step - loss: 4.4303 - acc: 0.1194\n","Epoch 39/600\n","63/63 [==============================] - 2s 29ms/step - loss: 4.3377 - acc: 0.1304\n","Epoch 40/600\n","63/63 [==============================] - 2s 29ms/step - loss: 4.4053 - acc: 0.1319\n","Epoch 41/600\n","63/63 [==============================] - 2s 29ms/step - loss: 4.2611 - acc: 0.1449\n","Epoch 42/600\n","63/63 [==============================] - 2s 29ms/step - loss: 4.1091 - acc: 0.1594\n","Epoch 43/600\n","63/63 [==============================] - 2s 29ms/step - loss: 3.9719 - acc: 0.1914\n","Epoch 44/600\n","63/63 [==============================] - 2s 29ms/step - loss: 4.0741 - acc: 0.1889\n","Epoch 45/600\n","63/63 [==============================] - 2s 29ms/step - loss: 3.8215 - acc: 0.2064\n","Epoch 46/600\n","63/63 [==============================] - 2s 29ms/step - loss: 3.8240 - acc: 0.2259\n","Epoch 47/600\n","63/63 [==============================] - 2s 30ms/step - loss: 3.7085 - acc: 0.2224\n","Epoch 48/600\n","63/63 [==============================] - 2s 29ms/step - loss: 3.6235 - acc: 0.2364\n","Epoch 49/600\n","63/63 [==============================] - 2s 29ms/step - loss: 3.5884 - acc: 0.2384\n","Epoch 50/600\n","63/63 [==============================] - 2s 29ms/step - loss: 3.5417 - acc: 0.2594\n","Epoch 51/600\n","63/63 [==============================] - 2s 30ms/step - loss: 3.4998 - acc: 0.2694\n","Epoch 52/600\n","63/63 [==============================] - 2s 30ms/step - loss: 3.4115 - acc: 0.2774\n","Epoch 53/600\n","63/63 [==============================] - 2s 30ms/step - loss: 3.6944 - acc: 0.2424\n","Epoch 54/600\n","63/63 [==============================] - 2s 30ms/step - loss: 3.4549 - acc: 0.2734\n","Epoch 55/600\n","63/63 [==============================] - 2s 29ms/step - loss: 3.3140 - acc: 0.2954\n","Epoch 56/600\n","63/63 [==============================] - 2s 29ms/step - loss: 3.1570 - acc: 0.3243\n","Epoch 57/600\n","63/63 [==============================] - 2s 29ms/step - loss: 3.2152 - acc: 0.3138\n","Epoch 58/600\n","63/63 [==============================] - 2s 29ms/step - loss: 3.0932 - acc: 0.3548\n","Epoch 59/600\n","63/63 [==============================] - 2s 29ms/step - loss: 3.0182 - acc: 0.3608\n","Epoch 60/600\n","63/63 [==============================] - 2s 29ms/step - loss: 2.9613 - acc: 0.3733\n","Epoch 61/600\n","63/63 [==============================] - 2s 29ms/step - loss: 3.0456 - acc: 0.3583\n","Epoch 62/600\n","63/63 [==============================] - 2s 29ms/step - loss: 2.8198 - acc: 0.3923\n","Epoch 63/600\n","63/63 [==============================] - 2s 29ms/step - loss: 2.8082 - acc: 0.4068\n","Epoch 64/600\n","63/63 [==============================] - 2s 28ms/step - loss: 2.7199 - acc: 0.4258\n","Epoch 65/600\n","63/63 [==============================] - 2s 29ms/step - loss: 2.5565 - acc: 0.4458\n","Epoch 66/600\n","63/63 [==============================] - 2s 29ms/step - loss: 2.5988 - acc: 0.4578\n","Epoch 67/600\n","63/63 [==============================] - 2s 29ms/step - loss: 2.6696 - acc: 0.4353\n","Epoch 68/600\n","63/63 [==============================] - 2s 29ms/step - loss: 2.5041 - acc: 0.4648\n","Epoch 69/600\n","63/63 [==============================] - 2s 29ms/step - loss: 2.4048 - acc: 0.4853\n","Epoch 70/600\n","63/63 [==============================] - 2s 29ms/step - loss: 2.4370 - acc: 0.4808\n","Epoch 71/600\n","63/63 [==============================] - 2s 29ms/step - loss: 2.4034 - acc: 0.4813\n","Epoch 72/600\n","63/63 [==============================] - 2s 29ms/step - loss: 2.3578 - acc: 0.4828\n","Epoch 73/600\n","63/63 [==============================] - 2s 29ms/step - loss: 2.3903 - acc: 0.5127\n","Epoch 74/600\n","63/63 [==============================] - 2s 29ms/step - loss: 2.3658 - acc: 0.5017\n","Epoch 75/600\n","63/63 [==============================] - 2s 29ms/step - loss: 2.4187 - acc: 0.5107\n","Epoch 76/600\n","63/63 [==============================] - 2s 29ms/step - loss: 2.0751 - acc: 0.5572\n","Epoch 77/600\n","63/63 [==============================] - 2s 34ms/step - loss: 2.1121 - acc: 0.5477\n","Epoch 78/600\n","63/63 [==============================] - 2s 30ms/step - loss: 2.0047 - acc: 0.5682\n","Epoch 79/600\n","63/63 [==============================] - 3s 40ms/step - loss: 2.1087 - acc: 0.5557\n","Epoch 80/600\n","63/63 [==============================] - 2s 32ms/step - loss: 1.9626 - acc: 0.5787\n","Epoch 81/600\n","63/63 [==============================] - 2s 30ms/step - loss: 2.1897 - acc: 0.5337\n","Epoch 82/600\n","63/63 [==============================] - 2s 30ms/step - loss: 2.0431 - acc: 0.5572\n","Epoch 83/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.9646 - acc: 0.5842\n","Epoch 84/600\n","63/63 [==============================] - 2s 30ms/step - loss: 2.0641 - acc: 0.5677\n","Epoch 85/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.9193 - acc: 0.5992\n","Epoch 86/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.8401 - acc: 0.6162\n","Epoch 87/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.8032 - acc: 0.6152\n","Epoch 88/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.7338 - acc: 0.6322\n","Epoch 89/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.8143 - acc: 0.6022\n","Epoch 90/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.9082 - acc: 0.5882\n","Epoch 91/600\n","63/63 [==============================] - 2s 29ms/step - loss: 2.0005 - acc: 0.5667\n","Epoch 92/600\n","63/63 [==============================] - 2s 29ms/step - loss: 2.0183 - acc: 0.5657\n","Epoch 93/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.8759 - acc: 0.5962\n","Epoch 94/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.8269 - acc: 0.5977\n","Epoch 95/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.7951 - acc: 0.6137\n","Epoch 96/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.6377 - acc: 0.6457\n","Epoch 97/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.6665 - acc: 0.6352\n","Epoch 98/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.7456 - acc: 0.6427\n","Epoch 99/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.6481 - acc: 0.6527\n","Epoch 100/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.6222 - acc: 0.6507\n","Epoch 101/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.5518 - acc: 0.6692\n","Epoch 102/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.5669 - acc: 0.6567\n","Epoch 103/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.5280 - acc: 0.6797\n","Epoch 104/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.4996 - acc: 0.6847\n","Epoch 105/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.5294 - acc: 0.6642\n","Epoch 106/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.4963 - acc: 0.6702\n","Epoch 107/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.6100 - acc: 0.6667\n","Epoch 108/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.5652 - acc: 0.6527\n","Epoch 109/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.5058 - acc: 0.6662\n","Epoch 110/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.5965 - acc: 0.6602\n","Epoch 111/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.6112 - acc: 0.6592\n","Epoch 112/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.4602 - acc: 0.6752\n","Epoch 113/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.5575 - acc: 0.6612\n","Epoch 114/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.4794 - acc: 0.6857\n","Epoch 115/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.3743 - acc: 0.7036\n","Epoch 116/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.5899 - acc: 0.6642\n","Epoch 117/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.3857 - acc: 0.7056\n","Epoch 118/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.4320 - acc: 0.7126\n","Epoch 119/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.3204 - acc: 0.7166\n","Epoch 120/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.3566 - acc: 0.7031\n","Epoch 121/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.3484 - acc: 0.7161\n","Epoch 122/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.3135 - acc: 0.7171\n","Epoch 123/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.2114 - acc: 0.7486\n","Epoch 124/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.1871 - acc: 0.7396\n","Epoch 125/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.3030 - acc: 0.7251\n","Epoch 126/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.3164 - acc: 0.7226\n","Epoch 127/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.1882 - acc: 0.7561\n","Epoch 128/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.2878 - acc: 0.7281\n","Epoch 129/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.2622 - acc: 0.7356\n","Epoch 130/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.2671 - acc: 0.7296\n","Epoch 131/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.1501 - acc: 0.7411\n","Epoch 132/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.2096 - acc: 0.7441\n","Epoch 133/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.2311 - acc: 0.7341\n","Epoch 134/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.2596 - acc: 0.7236\n","Epoch 135/600\n","63/63 [==============================] - 2s 28ms/step - loss: 1.2816 - acc: 0.7226\n","Epoch 136/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.2047 - acc: 0.7376\n","Epoch 137/600\n","63/63 [==============================] - 2s 28ms/step - loss: 1.2396 - acc: 0.7436\n","Epoch 138/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.0850 - acc: 0.7556\n","Epoch 139/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.1156 - acc: 0.7571\n","Epoch 140/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.1402 - acc: 0.7631\n","Epoch 141/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.1316 - acc: 0.7641\n","Epoch 142/600\n","63/63 [==============================] - 2s 28ms/step - loss: 1.1566 - acc: 0.7426\n","Epoch 143/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.0876 - acc: 0.7691\n","Epoch 144/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.0214 - acc: 0.7741\n","Epoch 145/600\n","63/63 [==============================] - 2s 35ms/step - loss: 1.1219 - acc: 0.7596\n","Epoch 146/600\n","63/63 [==============================] - 2s 36ms/step - loss: 1.1497 - acc: 0.7566\n","Epoch 147/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.0169 - acc: 0.7771\n","Epoch 148/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.0229 - acc: 0.7756\n","Epoch 149/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.1002 - acc: 0.7686\n","Epoch 150/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.1579 - acc: 0.7411\n","Epoch 151/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.1592 - acc: 0.7441\n","Epoch 152/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.2575 - acc: 0.7326\n","Epoch 153/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.0522 - acc: 0.7676\n","Epoch 154/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.0338 - acc: 0.7776\n","Epoch 155/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.1143 - acc: 0.7611\n","Epoch 156/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.1087 - acc: 0.7626\n","Epoch 157/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.0667 - acc: 0.7621\n","Epoch 158/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.1033 - acc: 0.7721\n","Epoch 159/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.0156 - acc: 0.7821\n","Epoch 160/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.0958 - acc: 0.7761\n","Epoch 161/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.9130 - acc: 0.7911\n","Epoch 162/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.9590 - acc: 0.7926\n","Epoch 163/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.9489 - acc: 0.7901\n","Epoch 164/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.0382 - acc: 0.7776\n","Epoch 165/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.9850 - acc: 0.7916\n","Epoch 166/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.0883 - acc: 0.7716\n","Epoch 167/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.9386 - acc: 0.7881\n","Epoch 168/600\n","63/63 [==============================] - 2s 29ms/step - loss: 1.0195 - acc: 0.7811\n","Epoch 169/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.9939 - acc: 0.7851\n","Epoch 170/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.0252 - acc: 0.7841\n","Epoch 171/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.0442 - acc: 0.7746\n","Epoch 172/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.6691 - acc: 0.6687\n","Epoch 173/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.6363 - acc: 0.6527\n","Epoch 174/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.5001 - acc: 0.6972\n","Epoch 175/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.3439 - acc: 0.7071\n","Epoch 176/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.3251 - acc: 0.7236\n","Epoch 177/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.1263 - acc: 0.7561\n","Epoch 178/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.9496 - acc: 0.7776\n","Epoch 179/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.9229 - acc: 0.7966\n","Epoch 180/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8910 - acc: 0.8056\n","Epoch 181/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.8845 - acc: 0.8041\n","Epoch 182/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.9267 - acc: 0.7951\n","Epoch 183/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.0551 - acc: 0.7901\n","Epoch 184/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8682 - acc: 0.8111\n","Epoch 185/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.9517 - acc: 0.7906\n","Epoch 186/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.9775 - acc: 0.7976\n","Epoch 187/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8604 - acc: 0.8106\n","Epoch 188/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8219 - acc: 0.8121\n","Epoch 189/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8648 - acc: 0.8081\n","Epoch 190/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.9183 - acc: 0.8036\n","Epoch 191/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.8307 - acc: 0.8236\n","Epoch 192/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8101 - acc: 0.8276\n","Epoch 193/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.8799 - acc: 0.8001\n","Epoch 194/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.9359 - acc: 0.7941\n","Epoch 195/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.0280 - acc: 0.7866\n","Epoch 196/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.8705 - acc: 0.8031\n","Epoch 197/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.7383 - acc: 0.8321\n","Epoch 198/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.9161 - acc: 0.8036\n","Epoch 199/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8615 - acc: 0.8066\n","Epoch 200/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.9156 - acc: 0.8056\n","Epoch 201/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.8604 - acc: 0.8126\n","Epoch 202/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7869 - acc: 0.8246\n","Epoch 203/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8110 - acc: 0.8171\n","Epoch 204/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8057 - acc: 0.8321\n","Epoch 205/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.9046 - acc: 0.8106\n","Epoch 206/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.9335 - acc: 0.8041\n","Epoch 207/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.8482 - acc: 0.8126\n","Epoch 208/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.9038 - acc: 0.8116\n","Epoch 209/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.9995 - acc: 0.7956\n","Epoch 210/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8286 - acc: 0.8231\n","Epoch 211/600\n","63/63 [==============================] - 2s 37ms/step - loss: 0.7021 - acc: 0.8451\n","Epoch 212/600\n","63/63 [==============================] - 2s 35ms/step - loss: 0.7684 - acc: 0.8291\n","Epoch 213/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.7464 - acc: 0.8346\n","Epoch 214/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7493 - acc: 0.8311\n","Epoch 215/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.7072 - acc: 0.8391\n","Epoch 216/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8284 - acc: 0.8206\n","Epoch 217/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.8444 - acc: 0.8171\n","Epoch 218/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.9197 - acc: 0.7991\n","Epoch 219/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.9067 - acc: 0.8076\n","Epoch 220/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.9522 - acc: 0.8016\n","Epoch 221/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.8083 - acc: 0.8301\n","Epoch 222/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.6996 - acc: 0.8491\n","Epoch 223/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.7853 - acc: 0.8331\n","Epoch 224/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8140 - acc: 0.8221\n","Epoch 225/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.9280 - acc: 0.8056\n","Epoch 226/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.8825 - acc: 0.8261\n","Epoch 227/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.7604 - acc: 0.8326\n","Epoch 228/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8792 - acc: 0.8161\n","Epoch 229/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.9860 - acc: 0.7796\n","Epoch 230/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.2392 - acc: 0.7481\n","Epoch 231/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.9549 - acc: 0.7931\n","Epoch 232/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.9631 - acc: 0.7996\n","Epoch 233/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.9092 - acc: 0.8096\n","Epoch 234/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7862 - acc: 0.8256\n","Epoch 235/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7992 - acc: 0.8266\n","Epoch 236/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.6959 - acc: 0.8516\n","Epoch 237/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7440 - acc: 0.8406\n","Epoch 238/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.8213 - acc: 0.8241\n","Epoch 239/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.9009 - acc: 0.8066\n","Epoch 240/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.8780 - acc: 0.8056\n","Epoch 241/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.7139 - acc: 0.8471\n","Epoch 242/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6921 - acc: 0.8581\n","Epoch 243/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6889 - acc: 0.8516\n","Epoch 244/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.6459 - acc: 0.8471\n","Epoch 245/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7749 - acc: 0.8411\n","Epoch 246/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.6816 - acc: 0.8511\n","Epoch 247/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.6709 - acc: 0.8501\n","Epoch 248/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7205 - acc: 0.8446\n","Epoch 249/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6971 - acc: 0.8476\n","Epoch 250/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.6367 - acc: 0.8631\n","Epoch 251/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6407 - acc: 0.8566\n","Epoch 252/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6591 - acc: 0.8521\n","Epoch 253/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6802 - acc: 0.8531\n","Epoch 254/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6805 - acc: 0.8541\n","Epoch 255/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6578 - acc: 0.8611\n","Epoch 256/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.7259 - acc: 0.8551\n","Epoch 257/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.9995 - acc: 0.8051\n","Epoch 258/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8650 - acc: 0.8331\n","Epoch 259/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8347 - acc: 0.8226\n","Epoch 260/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6935 - acc: 0.8466\n","Epoch 261/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.7258 - acc: 0.8496\n","Epoch 262/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.7551 - acc: 0.8406\n","Epoch 263/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7437 - acc: 0.8456\n","Epoch 264/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7112 - acc: 0.8506\n","Epoch 265/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.9956 - acc: 0.6587\n","Epoch 266/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.3657 - acc: 0.7381\n","Epoch 267/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.2558 - acc: 0.7506\n","Epoch 268/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.0080 - acc: 0.7916\n","Epoch 269/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.8640 - acc: 0.8206\n","Epoch 270/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8192 - acc: 0.8276\n","Epoch 271/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7017 - acc: 0.8436\n","Epoch 272/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6094 - acc: 0.8631\n","Epoch 273/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6822 - acc: 0.8551\n","Epoch 274/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6966 - acc: 0.8506\n","Epoch 275/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7124 - acc: 0.8426\n","Epoch 276/600\n","63/63 [==============================] - 2s 36ms/step - loss: 0.6335 - acc: 0.8496\n","Epoch 277/600\n","63/63 [==============================] - 3s 41ms/step - loss: 0.8365 - acc: 0.8346\n","Epoch 278/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7705 - acc: 0.8351\n","Epoch 279/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6636 - acc: 0.8486\n","Epoch 280/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5641 - acc: 0.8661\n","Epoch 281/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6998 - acc: 0.8516\n","Epoch 282/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6514 - acc: 0.8666\n","Epoch 283/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6339 - acc: 0.8616\n","Epoch 284/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6004 - acc: 0.8621\n","Epoch 285/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8264 - acc: 0.8296\n","Epoch 286/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.8416 - acc: 0.8141\n","Epoch 287/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8204 - acc: 0.8256\n","Epoch 288/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8044 - acc: 0.8246\n","Epoch 289/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7403 - acc: 0.8391\n","Epoch 290/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7014 - acc: 0.8421\n","Epoch 291/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8257 - acc: 0.8231\n","Epoch 292/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7664 - acc: 0.8351\n","Epoch 293/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.7455 - acc: 0.8426\n","Epoch 294/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7540 - acc: 0.8391\n","Epoch 295/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7013 - acc: 0.8471\n","Epoch 296/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7756 - acc: 0.8371\n","Epoch 297/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7996 - acc: 0.8266\n","Epoch 298/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8185 - acc: 0.8251\n","Epoch 299/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.7569 - acc: 0.8346\n","Epoch 300/600\n","63/63 [==============================] - 2s 31ms/step - loss: 1.3660 - acc: 0.7331\n","Epoch 301/600\n","63/63 [==============================] - 2s 31ms/step - loss: 1.1324 - acc: 0.7791\n","Epoch 302/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.9470 - acc: 0.7866\n","Epoch 303/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.9817 - acc: 0.8006\n","Epoch 304/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7546 - acc: 0.8276\n","Epoch 305/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6598 - acc: 0.8516\n","Epoch 306/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7148 - acc: 0.8511\n","Epoch 307/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6607 - acc: 0.8601\n","Epoch 308/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5746 - acc: 0.8706\n","Epoch 309/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6676 - acc: 0.8406\n","Epoch 310/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5309 - acc: 0.8751\n","Epoch 311/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6045 - acc: 0.8766\n","Epoch 312/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5202 - acc: 0.8841\n","Epoch 313/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5732 - acc: 0.8756\n","Epoch 314/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6480 - acc: 0.8701\n","Epoch 315/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.5570 - acc: 0.8741\n","Epoch 316/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6512 - acc: 0.8666\n","Epoch 317/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5377 - acc: 0.8761\n","Epoch 318/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5560 - acc: 0.8756\n","Epoch 319/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.6975 - acc: 0.8526\n","Epoch 320/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6625 - acc: 0.8551\n","Epoch 321/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7444 - acc: 0.8601\n","Epoch 322/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.7325 - acc: 0.8446\n","Epoch 323/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.9029 - acc: 0.8331\n","Epoch 324/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.9245 - acc: 0.8121\n","Epoch 325/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7208 - acc: 0.8451\n","Epoch 326/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6231 - acc: 0.8581\n","Epoch 327/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6642 - acc: 0.8576\n","Epoch 328/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5660 - acc: 0.8781\n","Epoch 329/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8291 - acc: 0.8356\n","Epoch 330/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6921 - acc: 0.8651\n","Epoch 331/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7050 - acc: 0.8526\n","Epoch 332/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6238 - acc: 0.8686\n","Epoch 333/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5878 - acc: 0.8726\n","Epoch 334/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6061 - acc: 0.8706\n","Epoch 335/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5052 - acc: 0.8851\n","Epoch 336/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6342 - acc: 0.8681\n","Epoch 337/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7351 - acc: 0.8551\n","Epoch 338/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6729 - acc: 0.8576\n","Epoch 339/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5962 - acc: 0.8811\n","Epoch 340/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.7673 - acc: 0.8521\n","Epoch 341/600\n","63/63 [==============================] - 2s 39ms/step - loss: 0.6998 - acc: 0.8616\n","Epoch 342/600\n","63/63 [==============================] - 2s 33ms/step - loss: 0.6572 - acc: 0.8681\n","Epoch 343/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6619 - acc: 0.8696\n","Epoch 344/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8140 - acc: 0.8481\n","Epoch 345/600\n","63/63 [==============================] - 2s 31ms/step - loss: 1.3902 - acc: 0.7481\n","Epoch 346/600\n","63/63 [==============================] - 2s 31ms/step - loss: 1.3019 - acc: 0.7731\n","Epoch 347/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.9855 - acc: 0.7946\n","Epoch 348/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.0630 - acc: 0.7981\n","Epoch 349/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8205 - acc: 0.8286\n","Epoch 350/600\n","63/63 [==============================] - 2s 29ms/step - loss: 0.8529 - acc: 0.8266\n","Epoch 351/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7296 - acc: 0.8586\n","Epoch 352/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6454 - acc: 0.8591\n","Epoch 353/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6302 - acc: 0.8681\n","Epoch 354/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7203 - acc: 0.8661\n","Epoch 355/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7108 - acc: 0.8616\n","Epoch 356/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6088 - acc: 0.8621\n","Epoch 357/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5577 - acc: 0.8701\n","Epoch 358/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7177 - acc: 0.8566\n","Epoch 359/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5152 - acc: 0.8886\n","Epoch 360/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5955 - acc: 0.8701\n","Epoch 361/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5470 - acc: 0.8801\n","Epoch 362/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5705 - acc: 0.8791\n","Epoch 363/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6400 - acc: 0.8701\n","Epoch 364/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7041 - acc: 0.8606\n","Epoch 365/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6025 - acc: 0.8731\n","Epoch 366/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7489 - acc: 0.8591\n","Epoch 367/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6071 - acc: 0.8716\n","Epoch 368/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6483 - acc: 0.8596\n","Epoch 369/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6128 - acc: 0.8751\n","Epoch 370/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5556 - acc: 0.8791\n","Epoch 371/600\n","63/63 [==============================] - 2s 30ms/step - loss: 1.1445 - acc: 0.7916\n","Epoch 372/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8294 - acc: 0.8401\n","Epoch 373/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.8278 - acc: 0.8546\n","Epoch 374/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7332 - acc: 0.8606\n","Epoch 375/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5782 - acc: 0.8816\n","Epoch 376/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4621 - acc: 0.8901\n","Epoch 377/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5351 - acc: 0.8901\n","Epoch 378/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6864 - acc: 0.8636\n","Epoch 379/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6310 - acc: 0.8796\n","Epoch 380/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6848 - acc: 0.8571\n","Epoch 381/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7747 - acc: 0.8481\n","Epoch 382/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6119 - acc: 0.8756\n","Epoch 383/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5800 - acc: 0.8756\n","Epoch 384/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5570 - acc: 0.8826\n","Epoch 385/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6224 - acc: 0.8816\n","Epoch 386/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.4827 - acc: 0.8936\n","Epoch 387/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5668 - acc: 0.8831\n","Epoch 388/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5401 - acc: 0.8946\n","Epoch 389/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6229 - acc: 0.8801\n","Epoch 390/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5960 - acc: 0.8776\n","Epoch 391/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6338 - acc: 0.8801\n","Epoch 392/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5904 - acc: 0.8831\n","Epoch 393/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6252 - acc: 0.8796\n","Epoch 394/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7638 - acc: 0.8541\n","Epoch 395/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7272 - acc: 0.8641\n","Epoch 396/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5464 - acc: 0.8821\n","Epoch 397/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6040 - acc: 0.8876\n","Epoch 398/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4840 - acc: 0.8916\n","Epoch 399/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6407 - acc: 0.8816\n","Epoch 400/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6765 - acc: 0.8621\n","Epoch 401/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6878 - acc: 0.8731\n","Epoch 402/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6240 - acc: 0.8841\n","Epoch 403/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6736 - acc: 0.8686\n","Epoch 404/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.7629 - acc: 0.8486\n","Epoch 405/600\n","63/63 [==============================] - 2s 37ms/step - loss: 0.7872 - acc: 0.8516\n","Epoch 406/600\n","63/63 [==============================] - 2s 37ms/step - loss: 0.7860 - acc: 0.8531\n","Epoch 407/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.7317 - acc: 0.8576\n","Epoch 408/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7707 - acc: 0.8496\n","Epoch 409/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6789 - acc: 0.8761\n","Epoch 410/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6548 - acc: 0.8771\n","Epoch 411/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5432 - acc: 0.8866\n","Epoch 412/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4760 - acc: 0.9055\n","Epoch 413/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.4458 - acc: 0.9090\n","Epoch 414/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5061 - acc: 0.8951\n","Epoch 415/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5317 - acc: 0.8951\n","Epoch 416/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6282 - acc: 0.8716\n","Epoch 417/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5406 - acc: 0.8891\n","Epoch 418/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4640 - acc: 0.9005\n","Epoch 419/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5241 - acc: 0.8961\n","Epoch 420/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5829 - acc: 0.8841\n","Epoch 421/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.7084 - acc: 0.8646\n","Epoch 422/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5735 - acc: 0.8816\n","Epoch 423/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6804 - acc: 0.8781\n","Epoch 424/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6121 - acc: 0.8816\n","Epoch 425/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.8030 - acc: 0.8721\n","Epoch 426/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5809 - acc: 0.8826\n","Epoch 427/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6148 - acc: 0.8836\n","Epoch 428/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5640 - acc: 0.8971\n","Epoch 429/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5853 - acc: 0.8851\n","Epoch 430/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5057 - acc: 0.9025\n","Epoch 431/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6086 - acc: 0.8891\n","Epoch 432/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6142 - acc: 0.8836\n","Epoch 433/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5822 - acc: 0.8906\n","Epoch 434/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6405 - acc: 0.8831\n","Epoch 435/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.7229 - acc: 0.8666\n","Epoch 436/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6373 - acc: 0.8751\n","Epoch 437/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5198 - acc: 0.8986\n","Epoch 438/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4973 - acc: 0.9030\n","Epoch 439/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5281 - acc: 0.8916\n","Epoch 440/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5315 - acc: 0.9020\n","Epoch 441/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4877 - acc: 0.8946\n","Epoch 442/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5828 - acc: 0.8906\n","Epoch 443/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5245 - acc: 0.8956\n","Epoch 444/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5484 - acc: 0.8946\n","Epoch 445/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5659 - acc: 0.8921\n","Epoch 446/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6050 - acc: 0.8896\n","Epoch 447/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5027 - acc: 0.9000\n","Epoch 448/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5644 - acc: 0.8996\n","Epoch 449/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4506 - acc: 0.9050\n","Epoch 450/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5238 - acc: 0.8996\n","Epoch 451/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6579 - acc: 0.8761\n","Epoch 452/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6754 - acc: 0.8751\n","Epoch 453/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6895 - acc: 0.8771\n","Epoch 454/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6670 - acc: 0.8851\n","Epoch 455/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6465 - acc: 0.8801\n","Epoch 456/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5209 - acc: 0.8996\n","Epoch 457/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5940 - acc: 0.8861\n","Epoch 458/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5449 - acc: 0.9010\n","Epoch 459/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5777 - acc: 0.8921\n","Epoch 460/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5543 - acc: 0.8976\n","Epoch 461/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5795 - acc: 0.8936\n","Epoch 462/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5597 - acc: 0.8956\n","Epoch 463/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6224 - acc: 0.8906\n","Epoch 464/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5139 - acc: 0.9015\n","Epoch 465/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6318 - acc: 0.8791\n","Epoch 466/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.7042 - acc: 0.8696\n","Epoch 467/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6701 - acc: 0.8796\n","Epoch 468/600\n","63/63 [==============================] - 2s 39ms/step - loss: 0.5567 - acc: 0.8991\n","Epoch 469/600\n","63/63 [==============================] - 2s 38ms/step - loss: 0.6653 - acc: 0.8861\n","Epoch 470/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5255 - acc: 0.8971\n","Epoch 471/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5217 - acc: 0.8931\n","Epoch 472/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5500 - acc: 0.8881\n","Epoch 473/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6156 - acc: 0.8881\n","Epoch 474/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5505 - acc: 0.8931\n","Epoch 475/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5064 - acc: 0.8986\n","Epoch 476/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4402 - acc: 0.9120\n","Epoch 477/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5113 - acc: 0.9110\n","Epoch 478/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6650 - acc: 0.8796\n","Epoch 479/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.7480 - acc: 0.8771\n","Epoch 480/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.7896 - acc: 0.8676\n","Epoch 481/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.7714 - acc: 0.8751\n","Epoch 482/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.7606 - acc: 0.8636\n","Epoch 483/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6373 - acc: 0.8886\n","Epoch 484/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5238 - acc: 0.9000\n","Epoch 485/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6825 - acc: 0.8826\n","Epoch 486/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6042 - acc: 0.8906\n","Epoch 487/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5344 - acc: 0.9035\n","Epoch 488/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5414 - acc: 0.8996\n","Epoch 489/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4610 - acc: 0.9105\n","Epoch 490/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4663 - acc: 0.9105\n","Epoch 491/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5335 - acc: 0.9065\n","Epoch 492/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4706 - acc: 0.9000\n","Epoch 493/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5069 - acc: 0.9015\n","Epoch 494/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5918 - acc: 0.8966\n","Epoch 495/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4581 - acc: 0.9080\n","Epoch 496/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6057 - acc: 0.8921\n","Epoch 497/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.6173 - acc: 0.8876\n","Epoch 498/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.4430 - acc: 0.9130\n","Epoch 499/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5289 - acc: 0.9055\n","Epoch 500/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5126 - acc: 0.9035\n","Epoch 501/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6442 - acc: 0.8856\n","Epoch 502/600\n","63/63 [==============================] - 2s 30ms/step - loss: 0.5245 - acc: 0.9050\n","Epoch 503/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4557 - acc: 0.9130\n","Epoch 504/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5411 - acc: 0.9080\n","Epoch 505/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5276 - acc: 0.9010\n","Epoch 506/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5184 - acc: 0.8976\n","Epoch 507/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4373 - acc: 0.9085\n","Epoch 508/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5308 - acc: 0.9055\n","Epoch 509/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5674 - acc: 0.8936\n","Epoch 510/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6414 - acc: 0.8896\n","Epoch 511/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5855 - acc: 0.8911\n","Epoch 512/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5137 - acc: 0.9090\n","Epoch 513/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4784 - acc: 0.9050\n","Epoch 514/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6768 - acc: 0.8821\n","Epoch 515/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5742 - acc: 0.8966\n","Epoch 516/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6143 - acc: 0.8846\n","Epoch 517/600\n","63/63 [==============================] - 2s 32ms/step - loss: 0.5853 - acc: 0.8946\n","Epoch 518/600\n","63/63 [==============================] - 2s 32ms/step - loss: 0.6111 - acc: 0.8976\n","Epoch 519/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6587 - acc: 0.8886\n","Epoch 520/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6517 - acc: 0.8791\n","Epoch 521/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5788 - acc: 0.8881\n","Epoch 522/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5406 - acc: 0.8981\n","Epoch 523/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4661 - acc: 0.9030\n","Epoch 524/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5073 - acc: 0.9025\n","Epoch 525/600\n","63/63 [==============================] - 2s 32ms/step - loss: 0.5260 - acc: 0.9055\n","Epoch 526/600\n","63/63 [==============================] - 2s 32ms/step - loss: 0.4792 - acc: 0.9075\n","Epoch 527/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.7945 - acc: 0.8751\n","Epoch 528/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.8420 - acc: 0.8566\n","Epoch 529/600\n","63/63 [==============================] - 2s 32ms/step - loss: 0.6280 - acc: 0.8906\n","Epoch 530/600\n","63/63 [==============================] - 2s 33ms/step - loss: 0.6548 - acc: 0.8841\n","Epoch 531/600\n","63/63 [==============================] - 3s 40ms/step - loss: 0.5442 - acc: 0.9060\n","Epoch 532/600\n","63/63 [==============================] - 2s 34ms/step - loss: 0.5422 - acc: 0.9020\n","Epoch 533/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5738 - acc: 0.8996\n","Epoch 534/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6913 - acc: 0.8811\n","Epoch 535/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5556 - acc: 0.9030\n","Epoch 536/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5707 - acc: 0.8936\n","Epoch 537/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6397 - acc: 0.8896\n","Epoch 538/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5501 - acc: 0.8991\n","Epoch 539/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4401 - acc: 0.9150\n","Epoch 540/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4755 - acc: 0.9145\n","Epoch 541/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4050 - acc: 0.9150\n","Epoch 542/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4347 - acc: 0.9205\n","Epoch 543/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4760 - acc: 0.9040\n","Epoch 544/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4575 - acc: 0.9135\n","Epoch 545/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4932 - acc: 0.9115\n","Epoch 546/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5503 - acc: 0.8961\n","Epoch 547/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5926 - acc: 0.9015\n","Epoch 548/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4129 - acc: 0.9200\n","Epoch 549/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4120 - acc: 0.9240\n","Epoch 550/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4150 - acc: 0.9225\n","Epoch 551/600\n","63/63 [==============================] - 2s 32ms/step - loss: 0.4827 - acc: 0.9100\n","Epoch 552/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6776 - acc: 0.8936\n","Epoch 553/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6040 - acc: 0.8911\n","Epoch 554/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6365 - acc: 0.8931\n","Epoch 555/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.9115 - acc: 0.8651\n","Epoch 556/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6303 - acc: 0.8886\n","Epoch 557/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.7289 - acc: 0.8926\n","Epoch 558/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6213 - acc: 0.8881\n","Epoch 559/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.7060 - acc: 0.8811\n","Epoch 560/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.7343 - acc: 0.8811\n","Epoch 561/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5893 - acc: 0.9080\n","Epoch 562/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4931 - acc: 0.9140\n","Epoch 563/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6638 - acc: 0.8906\n","Epoch 564/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.3563 - acc: 0.9325\n","Epoch 565/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.3632 - acc: 0.9250\n","Epoch 566/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.3477 - acc: 0.9295\n","Epoch 567/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5624 - acc: 0.9005\n","Epoch 568/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4441 - acc: 0.9185\n","Epoch 569/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.3662 - acc: 0.9285\n","Epoch 570/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4264 - acc: 0.9205\n","Epoch 571/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.3045 - acc: 0.9370\n","Epoch 572/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4652 - acc: 0.9160\n","Epoch 573/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5434 - acc: 0.8986\n","Epoch 574/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.6359 - acc: 0.8991\n","Epoch 575/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5306 - acc: 0.9110\n","Epoch 576/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4294 - acc: 0.9145\n","Epoch 577/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.3606 - acc: 0.9265\n","Epoch 578/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.3952 - acc: 0.9300\n","Epoch 579/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5425 - acc: 0.9035\n","Epoch 580/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.3177 - acc: 0.9340\n","Epoch 581/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4118 - acc: 0.9190\n","Epoch 582/600\n","63/63 [==============================] - 2s 32ms/step - loss: 0.4350 - acc: 0.9230\n","Epoch 583/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4124 - acc: 0.9270\n","Epoch 584/600\n","63/63 [==============================] - 2s 32ms/step - loss: 0.5160 - acc: 0.9040\n","Epoch 585/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4947 - acc: 0.9080\n","Epoch 586/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.7177 - acc: 0.8866\n","Epoch 587/600\n","63/63 [==============================] - 2s 32ms/step - loss: 0.7737 - acc: 0.8781\n","Epoch 588/600\n","63/63 [==============================] - 2s 32ms/step - loss: 0.6708 - acc: 0.8951\n","Epoch 589/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4945 - acc: 0.9110\n","Epoch 590/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4083 - acc: 0.9225\n","Epoch 591/600\n","63/63 [==============================] - 2s 32ms/step - loss: 0.5077 - acc: 0.9085\n","Epoch 592/600\n","63/63 [==============================] - 3s 41ms/step - loss: 0.5621 - acc: 0.9040\n","Epoch 593/600\n","63/63 [==============================] - 2s 37ms/step - loss: 0.5107 - acc: 0.9120\n","Epoch 594/600\n","63/63 [==============================] - 2s 32ms/step - loss: 0.4554 - acc: 0.9220\n","Epoch 595/600\n","63/63 [==============================] - 2s 32ms/step - loss: 0.6035 - acc: 0.9080\n","Epoch 596/600\n","63/63 [==============================] - 2s 32ms/step - loss: 0.6224 - acc: 0.8961\n","Epoch 597/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4591 - acc: 0.9185\n","Epoch 598/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5942 - acc: 0.9095\n","Epoch 599/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.4762 - acc: 0.9095\n","Epoch 600/600\n","63/63 [==============================] - 2s 31ms/step - loss: 0.5798 - acc: 0.9025\n"]}],"source":["# 6. 모델 데이터 세팅 (메모리 한계로 인해 일부만 사용)\n","user_data_train['user'] = user_data_train['user'].astype('int')\n","user_data_test['user'] = user_data_test['user'].astype('int')\n","train_tmp = user_data_train[(user_data_train.user >= 0)&(user_data_train.user <= 2000)]\n","test_tmp = user_data_test[(user_data_test.user >= 2980)&(user_data_test.user <= 3020)]\n","\n","# 7. 모델 학습\n","history = model.fit([tf.keras.preprocessing.sequence.pad_sequences(train_tmp['product'])+1e-10,\n","                     tf.keras.preprocessing.sequence.pad_sequences(train_tmp['order_dow'])+1e-10,\n","                     tf.keras.preprocessing.sequence.pad_sequences(train_tmp['order_hour_of_day'])+1e-10,\n","                     tf.keras.preprocessing.sequence.pad_sequences(train_tmp['day_since_prior_order'])+1e-10,\n","                     tf.keras.preprocessing.sequence.pad_sequences(train_tmp['buy_am'])+ 1e-10,\n","                    ], train_tmp['predict_labels'].values,\n","                    epochs=600)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dgm-rOKqeZIj"},"outputs":[],"source":["# 8. 모델 저장\n","model.save(data_dir+'output/candidate_generation.h5')"]},{"cell_type":"markdown","source":["* 테스트 데이터에 대해 후보군을 추출합니다."],"metadata":{"id":"HRFEHc0Rb3k5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YIJfwc1me8hA"},"outputs":[],"source":["# 9. 후보군 생성\n","pred = model.predict([tf.keras.preprocessing.sequence.pad_sequences(test_tmp['product'])+ 1e-10,\n","           tf.keras.preprocessing.sequence.pad_sequences(test_tmp['order_dow'])+ 1e-10,\n","           tf.keras.preprocessing.sequence.pad_sequences(test_tmp['order_hour_of_day'])+ 1e-10,\n","           tf.keras.preprocessing.sequence.pad_sequences(test_tmp['day_since_prior_order'])+ 1e-10,\n","           tf.keras.preprocessing.sequence.pad_sequences(test_tmp['buy_am'])+ 1e-10,\n","           ])\n","\n","N = 40 # 사용자별로 추천할 아이템 개수\n","k = np.sort((-pred).argsort()[:,:N])\n","k = k.flatten()\n","k[k>data_train[\"product\"].max()]=0\n","k = np.unique(k)"]},{"cell_type":"markdown","metadata":{"id":"FhCLbkLqfIvL"},"source":["# 4. 순위 모델"]},{"cell_type":"markdown","source":["* 전처리 과정에서 사용될 함수를 정의합니다."],"metadata":{"id":"LatTxs1GiMRv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dZkmxBiqJsut"},"outputs":[],"source":["def normalize_col(df,col_name): # 정규화 함수\n","    df[col_name] = (df[col_name] - df[col_name].min()) / (df[col_name].max() - df[col_name].min())\n","    return df\n","\n","def get_aisles(products, aisles): # 동일 중분류에 속한 아이템의 개수만큼 중분류를 반복해서 각 행에 붙여줌 \n","  def get_all_aisles(ai):\n","    active = [str(aisles_encoded[aisle]) for aisle, a in zip(aisles, ai) if a==1]\n","    if len(active) == 0: return '0'\n","    return ','.join((active))\n","  products['all_aisles'] = [get_all_aisles(ai) for ai in zip(*[products[aisle] for aisle in aisles])]"]},{"cell_type":"markdown","source":["* 후보군 상품을 사용해 순위모델 데이터를 전처리합니다"],"metadata":{"id":"9J0yF0RJiPmh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Seplg-xUZf41"},"outputs":[],"source":["# 1. 데이터 준비\n","product_m = pd_clac.copy()\n","product_m.columns = ['product_id', 'pd_nm', 'department', 'aisle']\n","product_enc = pd.get_dummies(product_m, columns=['aisle'], prefix=[None])\n","product_enc.columns = ['product_id', 'product_name', 'department'] + list(product_enc.columns[3:])\n","\n","# 2. 아이템 속성 전처리\n","aisle_cols = pd_clac['clac_mcls_nm'].values.tolist()\n","type(aisle_cols)\n","aisles_encoded = {x: i for i, x in enumerate(aisle_cols)}\n","get_aisles(product_enc, aisle_cols)\n","\n","# 3. 후보군 관련 데이터 생성\n","ratings_train = data_train[['product_id', 'product', 'reordered', 'user_id', 'user']]\n","ratings_test = data_test[['product_id', 'product', 'reordered', 'user_id', 'user']]\n","\n","product_data = product_enc.set_index(['product_id']).sort_index()\n","product_data = product_data.reset_index().loc[k+1]\n","pd_nms = product_enc[\"product_name\"].unique().tolist()\n","pd_name2pd_name_encoded = {x: i for i, x in enumerate(pd_nms)}\n","product_data[\"pd_name_d\"] = product_data[\"product_name\"].map(pd_name2pd_name_encoded)\n","\n","# 4. 모델용 데이터 생성\n","new_data_train = product_data.merge(ratings_train, on='product_id') # rating 추가\n","new_data_test = product_data.merge(ratings_test, on='product_id') # rating 추가\n","for_merge = pd.concat([data_train_tmp, data_test_tmp], axis = 0)[['user', 'gender', 'region', 'ages']].drop_duplicates() # user별 정보 테이블"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pITMQVY5Xu0R"},"outputs":[],"source":["def generate_final_data(typ):\n","\n","  globals()['new_data_'+typ] = globals()['new_data_'+typ][['product', 'user', 'reordered', 'all_aisles', 'pd_name_d']]\n","  # product : product_id의 int매핑값, user : user_id의 매핑값, reordered : 재주문된 아이템인지, all_aisles : 같은 aisle에 속한 상품, pd_name_d, 소분류명 매핑값\n","  globals()['new_data_'+typ]['product_type'] = np.where(globals()['new_data_'+typ]['reordered'] ==1, 'like', 'dislike')\n","  # product_type : 재구매했는지 여부\n","\n","  globals()['product_list_'+typ] = globals()['new_data_'+typ].groupby(['user','product_type'])['product'].apply(list).reset_index()\n","  # product_list_typ :. user와 product_type으로 그룹화하여, product들을 리스트로 모아놓음\n","\n","  globals()['aisle_list_'+typ] = globals()['new_data_'+typ].groupby(['user'])['all_aisles'].unique().apply(list).reset_index()\n","  globals()['aisle_list_'+typ]['all_aisles'] = globals()['aisle_list_'+typ]['all_aisles'].apply(lambda x: list(set(','.join(x))) )\n","  globals()['aisle_list_'+typ]['all_aisles'] = globals()['aisle_list_'+typ]['all_aisles'].apply(lambda x: [ x for x in x if x.isdigit() ])\n","  # aisle_list_typ : user별로 구매한 중분류들을 모아놓음\n","  \n","  globals()['pd_name_list_'+typ] = globals()['new_data_'+typ].groupby(['user'])['pd_name_d'].apply(list).reset_index()\n","  # pd_name_list_typ : user별로 구매한 소분류들을 모아놓음\n","\n","  globals()['dataset_'+typ] = globals()['product_list_'+typ].pivot(index='user', columns='product_type', values='product').reset_index()\n","  globals()['dataset_'+typ].fillna(globals()['new_data_'+typ][\"product\"].max()+1, inplace=True)\n","  globals()['dataset_'+typ]['like'] =globals()['dataset_'+typ]['like'].apply(lambda x: x if type(x) is list else [])\n","  globals()['dataset_'+typ]['dislike'] =globals()['dataset_'+typ]['dislike'].apply(lambda x: x if type(x) is list else [])\n","  globals()['dataset_'+typ] = pd.merge(globals()['dataset_'+typ], globals()['pd_name_list_'+typ], how='left', on='user')\n","  globals()['dataset_'+typ] = pd.merge(globals()['dataset_'+typ], globals()['aisle_list_'+typ], how='left', on='user')\n","  globals()['dataset_'+typ] = pd.merge(globals()['dataset_'+typ], for_merge, how='left', on='user').reset_index(drop=True)\n","\n","  # gender, region, ages 리스트 변환\n","  for i in ['gender', 'region', 'ages']:\n","    globals()['dataset_'+typ][i] = [[x] for x in list(globals()['dataset_'+typ][i])]\n","\n","  # predict_label 채워넣기\n","  globals()['dataset_'+typ]['predict_labels'] = globals()['dataset_'+typ]['like'].apply(lambda x: int(random.uniform(1,globals()['new_data_'+typ][\"product\"].max()))) \n","\n","  # 채워넣은 결측치는 추후 대체 예정\n","  globals()['dataset_'+typ]['like'] = globals()['dataset_'+typ]['like'].apply(lambda x: [globals()['new_data_'+typ][\"product\"].max()+1] if x == [] else x)\n","  globals()['dataset_'+typ]['dislike'] = globals()['dataset_'+typ]['dislike'].apply(lambda x: [globals()['new_data_'+typ][\"product\"].max()+1] if x == [] else x)\n","\n","\n","# 5. 모델 입력 데이터 생성 및 저장\n","generate_final_data('train')\n","generate_final_data('test')\n","\n","dataset_train.to_csv(data_dir+'output/dataset_train.csv', index=False)\n","dataset_test.to_csv(data_dir+'output/dataset_test.csv', index=False)"]},{"cell_type":"markdown","source":["* 훈련에 사용할 데이터를 정의합니다."],"metadata":{"id":"74hORq1ai8rO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"r7EMRPUEvL1x"},"outputs":[],"source":["# 6. 입력 데이터 추출 (메모리 한계로 인해 일부만 사용)\n","dataset_train['user'] = dataset_train['user'].astype('int')\n","dataset_test['user'] = dataset_test['user'].astype('int')\n","\n","tmp_train_r = dataset_train[(dataset_train.index >= 0)&(dataset_train.index <= 2000)]\n","tmp_test_r = dataset_test[(dataset_test.index >= 0)&(dataset_test.index <= 40)]"]},{"cell_type":"code","source":["tmp_train_r"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"49_Zl5fE1hbF","executionInfo":{"status":"ok","timestamp":1660301661749,"user_tz":-540,"elapsed":15,"user":{"displayName":"안도현","userId":"08395434251954901727"}},"outputId":"d0891dba-09b3-4740-e5a7-9095503b0c7c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      user                                   dislike  \\\n","0        0       [197, 197, 197, 197, 197, 197, 824]   \n","1        1  [197, 197, 197, 197, 197, 197, 122, 259]   \n","2        3                                [317, 122]   \n","3        5   [741, 479, 479, 402, 402, 24, 562, 662]   \n","4        9                                     [479]   \n","...    ...                                       ...   \n","1996  3931                                     [317]   \n","1997  3933                                [122, 259]   \n","1998  3936                                [275, 259]   \n","1999  3937                                     [122]   \n","2000  3938                 [197, 122, 122, 122, 416]   \n","\n","                                like  \\\n","0                             [1358]   \n","1                             [1358]   \n","2                             [1358]   \n","3     [479, 479, 479, 479, 479, 402]   \n","4                             [1358]   \n","...                              ...   \n","1996                          [1358]   \n","1997                          [1358]   \n","1998                          [1358]   \n","1999                          [1358]   \n","2000                          [1358]   \n","\n","                                              pd_name_d  \\\n","0                   [322, 322, 322, 322, 322, 322, 929]   \n","1              [322, 322, 322, 322, 322, 322, 934, 946]   \n","2                                            [223, 934]   \n","3     [907, 947, 947, 947, 947, 947, 947, 947, 951, ...   \n","4                                                 [947]   \n","...                                                 ...   \n","1996                                              [223]   \n","1997                                         [934, 946]   \n","1998                                         [940, 946]   \n","1999                                              [934]   \n","2000                          [322, 934, 934, 934, 945]   \n","\n","                 all_aisles gender region ages  predict_labels  \n","0              [9, 3, 2, 6]    [0]    [0]  [0]             245  \n","1     [4, 8, 9, 3, 2, 7, 6]    [0]    [1]  [0]            1023  \n","2           [8, 9, 3, 2, 6]    [0]    [2]  [0]             998  \n","3     [0, 5, 4, 8, 9, 3, 6]    [1]    [2]  [0]            1330  \n","4                 [5, 9, 4]    [0]    [4]  [0]             529  \n","...                     ...    ...    ...  ...             ...  \n","1996                 [2, 6]    [0]    [1]  [1]             948  \n","1997        [4, 8, 9, 3, 7]    [0]    [8]  [0]             704  \n","1998              [9, 4, 7]    [0]    [1]  [0]              38  \n","1999              [3, 9, 8]    [1]    [4]  [3]             958  \n","2000  [4, 8, 9, 3, 2, 7, 6]    [0]    [4]  [0]             706  \n","\n","[2001 rows x 9 columns]"],"text/html":["\n","  <div id=\"df-f099a8e9-1b71-4c18-943d-0e781e65f367\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user</th>\n","      <th>dislike</th>\n","      <th>like</th>\n","      <th>pd_name_d</th>\n","      <th>all_aisles</th>\n","      <th>gender</th>\n","      <th>region</th>\n","      <th>ages</th>\n","      <th>predict_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>[197, 197, 197, 197, 197, 197, 824]</td>\n","      <td>[1358]</td>\n","      <td>[322, 322, 322, 322, 322, 322, 929]</td>\n","      <td>[9, 3, 2, 6]</td>\n","      <td>[0]</td>\n","      <td>[0]</td>\n","      <td>[0]</td>\n","      <td>245</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>[197, 197, 197, 197, 197, 197, 122, 259]</td>\n","      <td>[1358]</td>\n","      <td>[322, 322, 322, 322, 322, 322, 934, 946]</td>\n","      <td>[4, 8, 9, 3, 2, 7, 6]</td>\n","      <td>[0]</td>\n","      <td>[1]</td>\n","      <td>[0]</td>\n","      <td>1023</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>[317, 122]</td>\n","      <td>[1358]</td>\n","      <td>[223, 934]</td>\n","      <td>[8, 9, 3, 2, 6]</td>\n","      <td>[0]</td>\n","      <td>[2]</td>\n","      <td>[0]</td>\n","      <td>998</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5</td>\n","      <td>[741, 479, 479, 402, 402, 24, 562, 662]</td>\n","      <td>[479, 479, 479, 479, 479, 402]</td>\n","      <td>[907, 947, 947, 947, 947, 947, 947, 947, 951, ...</td>\n","      <td>[0, 5, 4, 8, 9, 3, 6]</td>\n","      <td>[1]</td>\n","      <td>[2]</td>\n","      <td>[0]</td>\n","      <td>1330</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>9</td>\n","      <td>[479]</td>\n","      <td>[1358]</td>\n","      <td>[947]</td>\n","      <td>[5, 9, 4]</td>\n","      <td>[0]</td>\n","      <td>[4]</td>\n","      <td>[0]</td>\n","      <td>529</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1996</th>\n","      <td>3931</td>\n","      <td>[317]</td>\n","      <td>[1358]</td>\n","      <td>[223]</td>\n","      <td>[2, 6]</td>\n","      <td>[0]</td>\n","      <td>[1]</td>\n","      <td>[1]</td>\n","      <td>948</td>\n","    </tr>\n","    <tr>\n","      <th>1997</th>\n","      <td>3933</td>\n","      <td>[122, 259]</td>\n","      <td>[1358]</td>\n","      <td>[934, 946]</td>\n","      <td>[4, 8, 9, 3, 7]</td>\n","      <td>[0]</td>\n","      <td>[8]</td>\n","      <td>[0]</td>\n","      <td>704</td>\n","    </tr>\n","    <tr>\n","      <th>1998</th>\n","      <td>3936</td>\n","      <td>[275, 259]</td>\n","      <td>[1358]</td>\n","      <td>[940, 946]</td>\n","      <td>[9, 4, 7]</td>\n","      <td>[0]</td>\n","      <td>[1]</td>\n","      <td>[0]</td>\n","      <td>38</td>\n","    </tr>\n","    <tr>\n","      <th>1999</th>\n","      <td>3937</td>\n","      <td>[122]</td>\n","      <td>[1358]</td>\n","      <td>[934]</td>\n","      <td>[3, 9, 8]</td>\n","      <td>[1]</td>\n","      <td>[4]</td>\n","      <td>[3]</td>\n","      <td>958</td>\n","    </tr>\n","    <tr>\n","      <th>2000</th>\n","      <td>3938</td>\n","      <td>[197, 122, 122, 122, 416]</td>\n","      <td>[1358]</td>\n","      <td>[322, 934, 934, 934, 945]</td>\n","      <td>[4, 8, 9, 3, 2, 7, 6]</td>\n","      <td>[0]</td>\n","      <td>[4]</td>\n","      <td>[0]</td>\n","      <td>706</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2001 rows × 9 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f099a8e9-1b71-4c18-943d-0e781e65f367')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f099a8e9-1b71-4c18-943d-0e781e65f367 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f099a8e9-1b71-4c18-943d-0e781e65f367');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["* 훈련에 사용할 파라미터를 정의합니다."],"metadata":{"id":"7Qi83FsGi_HN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uvxzi6csKZnm"},"outputs":[],"source":["# 7. 하이퍼파라미터 정의\n","EMBEDDING_DIMS = 64\n","DENSE_UNITS = 2048 # dense layer 뉴런 개수\n","DROPOUT_PCT = 0.1 # dropout\n","ALPHA = 0.1\n","NUM_CLASSES = data_train[\"product\"].max() + 1 # 큰거 기준\n","LEARNING_RATE = 0.01"]},{"cell_type":"markdown","source":["* 훈련에 사용할 모델을 정의합니다."],"metadata":{"id":"d0ChFcxkjE_U"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xTruVSmIKiW-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660301662301,"user_tz":-540,"elapsed":562,"user":{"displayName":"안도현","userId":"08395434251954901727"}},"outputId":"4152baca-9557-4941-d287-7a109a031613"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," product_name (InputLayer)      [(None, None)]       0           []                               \n","                                                                                                  \n"," like (InputLayer)              [(None, None)]       0           []                               \n","                                                                                                  \n"," dislike (InputLayer)           [(None, None)]       0           []                               \n","                                                                                                  \n"," aisle (InputLayer)             [(None, None)]       0           []                               \n","                                                                                                  \n"," gender (InputLayer)            [(None, None)]       0           []                               \n","                                                                                                  \n"," region (InputLayer)            [(None, None)]       0           []                               \n","                                                                                                  \n"," ages (InputLayer)              [(None, None)]       0           []                               \n","                                                                                                  \n"," features_embeddings (Embedding  (None, None, 64)    89664       ['product_name[0][0]']           \n"," )                                                                                                \n","                                                                                                  \n"," labels_embeddings (Embedding)  (None, None, 64)     89664       ['like[0][0]',                   \n","                                                                  'dislike[0][0]',                \n","                                                                  'aisle[0][0]',                  \n","                                                                  'gender[0][0]',                 \n","                                                                  'region[0][0]',                 \n","                                                                  'ages[0][0]']                   \n","                                                                                                  \n"," l2_norm_1 (L2NormLayer)        (None, None, 64)     0           ['features_embeddings[0][0]',    \n","                                                                  'labels_embeddings[0][0]',      \n","                                                                  'labels_embeddings[1][0]',      \n","                                                                  'labels_embeddings[2][0]',      \n","                                                                  'labels_embeddings[3][0]',      \n","                                                                  'labels_embeddings[4][0]',      \n","                                                                  'labels_embeddings[5][0]']      \n","                                                                                                  \n"," aggregate_embeddings (MaskedEm  (None, 64)          0           ['l2_norm_1[0][0]',              \n"," beddingsAggregatorLayer)                                         'l2_norm_1[1][0]',              \n","                                                                  'l2_norm_1[2][0]',              \n","                                                                  'l2_norm_1[3][0]',              \n","                                                                  'l2_norm_1[4][0]',              \n","                                                                  'l2_norm_1[5][0]',              \n","                                                                  'l2_norm_1[6][0]']              \n","                                                                                                  \n"," concatenate_1 (Concatenate)    (None, 448)          0           ['aggregate_embeddings[0][0]',   \n","                                                                  'aggregate_embeddings[1][0]',   \n","                                                                  'aggregate_embeddings[2][0]',   \n","                                                                  'aggregate_embeddings[3][0]',   \n","                                                                  'aggregate_embeddings[4][0]',   \n","                                                                  'aggregate_embeddings[5][0]',   \n","                                                                  'aggregate_embeddings[6][0]']   \n","                                                                                                  \n"," dense_1 (Dense)                (None, 2048)         919552      ['concatenate_1[0][0]']          \n","                                                                                                  \n"," dense_1_relu (ReLU)            (None, 2048)         0           ['dense_1[0][0]']                \n","                                                                                                  \n"," dense_2 (Dense)                (None, 2048)         4196352     ['dense_1_relu[0][0]']           \n","                                                                                                  \n"," dense_2_relu (ReLU)            (None, 2048)         0           ['dense_2[0][0]']                \n","                                                                                                  \n"," dense_3 (Dense)                (None, 2048)         4196352     ['dense_2_relu[0][0]']           \n","                                                                                                  \n"," dense_3_relu (ReLU)            (None, 2048)         0           ['dense_3[0][0]']                \n","                                                                                                  \n"," dense_3_batch_norm (BatchNorma  (None, 2048)        8192        ['dense_3_relu[0][0]']           \n"," lization)                                                                                        \n","                                                                                                  \n"," dense_output (Dense)           (None, 1401)         2870649     ['dense_3_batch_norm[0][0]']     \n","                                                                                                  \n","==================================================================================================\n","Total params: 12,370,425\n","Trainable params: 12,366,329\n","Non-trainable params: 4,096\n","__________________________________________________________________________________________________\n"]}],"source":["# 8. 딥러닝 모델 구성\n","# 8-1. 입력 데이터 및 임베딩, 레이어 정의\n","input_name = tf.keras.Input(shape=(None, ), name='product_name')\n","inp_item_liked = tf.keras.layers.Input(shape=(None,), name='like')\n","inp_item_disliked = tf.keras.layers.Input(shape=(None,), name='dislike')\n","input_aisle = tf.keras.Input(shape=(None, ), name='aisle')\n","input_gender = tf.keras.Input(shape=(None, ), name='gender')\n","input_region = tf.keras.Input(shape=(None, ), name='region')\n","input_ages = tf.keras.Input(shape=(None, ), name='ages')\n","\n","features_embedding_layer = tf.keras.layers.Embedding(input_dim=NUM_CLASSES, output_dim=EMBEDDING_DIMS,  mask_zero=True, trainable=True, name='features_embeddings')\n","labels_embedding_layer = tf.keras.layers.Embedding(input_dim=NUM_CLASSES, output_dim=EMBEDDING_DIMS, mask_zero=True, trainable=True, name='labels_embeddings')\n","avg_embeddings = MaskedEmbeddingsAggregatorLayer(agg_mode='mean', name='aggregate_embeddings')\n","\n","dense_1 = tf.keras.layers.Dense(units=DENSE_UNITS, name='dense_1')\n","dense_2 = tf.keras.layers.Dense(units=DENSE_UNITS, name='dense_2')\n","dense_3 = tf.keras.layers.Dense(units=DENSE_UNITS, name='dense_3')\n","l2_norm_1 = L2NormLayer(name='l2_norm_1')\n","dense_output = tf.keras.layers.Dense(NUM_CLASSES, activation=tf.nn.softmax, name='dense_output')\n","\n","# 8-2. 임베딩 벡터 구성\n","features_embeddings = features_embedding_layer(input_name)\n","l2_norm_features = l2_norm_1(features_embeddings)\n","avg_features = avg_embeddings(l2_norm_features)\n","\n","labels_liked_embeddings = labels_embedding_layer(inp_item_liked)\n","l2_norm_liked = l2_norm_1(labels_liked_embeddings)\n","avg_liked = avg_embeddings(l2_norm_liked)\n","\n","labels_disliked_embeddings = labels_embedding_layer(inp_item_disliked)\n","l2_norm_disliked = l2_norm_1(labels_disliked_embeddings)\n","avg_disliked = avg_embeddings(l2_norm_disliked)\n","\n","labels_aisle_embeddings = labels_embedding_layer(input_aisle)\n","l2_norm_aisle = l2_norm_1(labels_aisle_embeddings)\n","avg_aisle = avg_embeddings(l2_norm_aisle)\n","\n","labels_gender_embeddings = labels_embedding_layer(input_gender)\n","l2_norm_gender = l2_norm_1(labels_gender_embeddings)\n","avg_gender = avg_embeddings(l2_norm_gender)\n","\n","labels_region_embeddings = labels_embedding_layer(input_region)\n","l2_norm_region = l2_norm_1(labels_region_embeddings)\n","avg_region = avg_embeddings(l2_norm_region)\n","\n","labels_ages_embeddings = labels_embedding_layer(input_ages)\n","l2_norm_ages = l2_norm_1(labels_ages_embeddings)\n","avg_ages = avg_embeddings(l2_norm_ages)\n","\n","\n","# 8-3. 임베딩 벡터들 연결\n","concat_inputs = tf.keras.layers.Concatenate(axis=1)([avg_features,\n","                                                     avg_liked,\n","                                                     avg_disliked,\n","                                                     avg_aisle,\n","                                                     avg_gender,\n","                                                     avg_region,\n","                                                     avg_ages\n","                                                     ])\n","# 8-4. Dense Layer 구성\n","dense_1_features = dense_1(concat_inputs)\n","dense_1_relu = tf.keras.layers.ReLU(name='dense_1_relu')(dense_1_features)\n","dense_1_batch_norm = tf.keras.layers.BatchNormalization(name='dense_1_batch_norm')(dense_1_relu)\n","\n","dense_2_features = dense_2(dense_1_relu)\n","dense_2_relu = tf.keras.layers.ReLU(name='dense_2_relu')(dense_2_features)\n","dense_2_batch_norm = tf.keras.layers.BatchNormalization(name='dense_2_batch_norm')(dense_2_relu)\n","\n","dense_3_features = dense_3(dense_2_relu)\n","dense_3_relu = tf.keras.layers.ReLU(name='dense_3_relu')(dense_3_features)\n","dense_3_batch_norm = tf.keras.layers.BatchNormalization(name='dense_3_batch_norm')(dense_3_relu)\n","outputs = dense_output(dense_3_batch_norm)\n","\n","# 8-5. Optimizer 정의\n","optimiser = tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE)\n","\n","# 8-6. 모델 준비\n","model_rank = tf.keras.models.Model(\n","    inputs=[input_name, \n","            inp_item_liked, \n","            inp_item_disliked,\n","            input_aisle,\n","            input_gender,\n","            input_region,\n","            input_ages\n","            ],\n","    outputs=[outputs]\n",")\n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n","model_rank.compile(optimizer=optimiser, loss='sparse_categorical_crossentropy', metrics=['acc'])\n","model_rank.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OPUQB823KrOd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bc41a558-ce9e-401e-b3e4-70c008135627","executionInfo":{"status":"ok","timestamp":1660301986080,"user_tz":-540,"elapsed":323782,"user":{"displayName":"안도현","userId":"08395434251954901727"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/300\n","63/63 [==============================] - 3s 17ms/step - loss: 7.3082 - acc: 0.0000e+00\n","Epoch 2/300\n","63/63 [==============================] - 1s 16ms/step - loss: 7.0648 - acc: 0.0035\n","Epoch 3/300\n","63/63 [==============================] - 1s 17ms/step - loss: 6.8403 - acc: 0.0110\n","Epoch 4/300\n","63/63 [==============================] - 1s 17ms/step - loss: 6.6232 - acc: 0.0210\n","Epoch 5/300\n","63/63 [==============================] - 1s 17ms/step - loss: 6.4136 - acc: 0.0270\n","Epoch 6/300\n","63/63 [==============================] - 1s 17ms/step - loss: 6.2205 - acc: 0.0320\n","Epoch 7/300\n","63/63 [==============================] - 1s 16ms/step - loss: 6.0275 - acc: 0.0330\n","Epoch 8/300\n","63/63 [==============================] - 1s 17ms/step - loss: 5.8684 - acc: 0.0375\n","Epoch 9/300\n","63/63 [==============================] - 1s 16ms/step - loss: 5.6982 - acc: 0.0435\n","Epoch 10/300\n","63/63 [==============================] - 1s 16ms/step - loss: 5.5535 - acc: 0.0445\n","Epoch 11/300\n","63/63 [==============================] - 1s 16ms/step - loss: 5.4100 - acc: 0.0465\n","Epoch 12/300\n","63/63 [==============================] - 1s 16ms/step - loss: 5.2816 - acc: 0.0500\n","Epoch 13/300\n","63/63 [==============================] - 1s 17ms/step - loss: 5.1691 - acc: 0.0550\n","Epoch 14/300\n","63/63 [==============================] - 1s 17ms/step - loss: 5.0444 - acc: 0.0545\n","Epoch 15/300\n","63/63 [==============================] - 1s 16ms/step - loss: 4.9547 - acc: 0.0550\n","Epoch 16/300\n","63/63 [==============================] - 1s 17ms/step - loss: 4.8559 - acc: 0.0535\n","Epoch 17/300\n","63/63 [==============================] - 1s 17ms/step - loss: 4.7601 - acc: 0.0550\n","Epoch 18/300\n","63/63 [==============================] - 1s 17ms/step - loss: 4.6841 - acc: 0.0625\n","Epoch 19/300\n","63/63 [==============================] - 1s 17ms/step - loss: 4.6129 - acc: 0.0655\n","Epoch 20/300\n","63/63 [==============================] - 1s 17ms/step - loss: 4.5512 - acc: 0.0650\n","Epoch 21/300\n","63/63 [==============================] - 1s 17ms/step - loss: 4.4911 - acc: 0.0675\n","Epoch 22/300\n","63/63 [==============================] - 1s 17ms/step - loss: 4.4249 - acc: 0.0630\n","Epoch 23/300\n","63/63 [==============================] - 1s 17ms/step - loss: 4.3694 - acc: 0.0650\n","Epoch 24/300\n","63/63 [==============================] - 1s 17ms/step - loss: 4.3146 - acc: 0.0695\n","Epoch 25/300\n","63/63 [==============================] - 1s 17ms/step - loss: 4.2588 - acc: 0.0700\n","Epoch 26/300\n","63/63 [==============================] - 1s 16ms/step - loss: 4.2225 - acc: 0.0775\n","Epoch 27/300\n","63/63 [==============================] - 1s 17ms/step - loss: 4.1778 - acc: 0.0775\n","Epoch 28/300\n","63/63 [==============================] - 1s 16ms/step - loss: 4.1230 - acc: 0.0770\n","Epoch 29/300\n","63/63 [==============================] - 1s 17ms/step - loss: 4.1002 - acc: 0.0825\n","Epoch 30/300\n","63/63 [==============================] - 1s 16ms/step - loss: 4.0469 - acc: 0.0850\n","Epoch 31/300\n","63/63 [==============================] - 1s 17ms/step - loss: 4.0184 - acc: 0.0885\n","Epoch 32/300\n","63/63 [==============================] - 1s 16ms/step - loss: 3.9799 - acc: 0.0890\n","Epoch 33/300\n","63/63 [==============================] - 1s 17ms/step - loss: 3.9328 - acc: 0.0925\n","Epoch 34/300\n","63/63 [==============================] - 1s 17ms/step - loss: 3.9194 - acc: 0.0965\n","Epoch 35/300\n","63/63 [==============================] - 1s 17ms/step - loss: 3.8680 - acc: 0.1054\n","Epoch 36/300\n","63/63 [==============================] - 1s 17ms/step - loss: 3.8408 - acc: 0.1034\n","Epoch 37/300\n","63/63 [==============================] - 1s 16ms/step - loss: 3.7923 - acc: 0.1094\n","Epoch 38/300\n","63/63 [==============================] - 1s 17ms/step - loss: 3.7685 - acc: 0.1074\n","Epoch 39/300\n","63/63 [==============================] - 1s 16ms/step - loss: 3.7317 - acc: 0.1159\n","Epoch 40/300\n","63/63 [==============================] - 1s 16ms/step - loss: 3.6892 - acc: 0.1199\n","Epoch 41/300\n","63/63 [==============================] - 1s 17ms/step - loss: 3.6599 - acc: 0.1264\n","Epoch 42/300\n","63/63 [==============================] - 1s 17ms/step - loss: 3.6366 - acc: 0.1269\n","Epoch 43/300\n","63/63 [==============================] - 1s 17ms/step - loss: 3.5975 - acc: 0.1239\n","Epoch 44/300\n","63/63 [==============================] - 1s 17ms/step - loss: 3.5578 - acc: 0.1359\n","Epoch 45/300\n","63/63 [==============================] - 1s 17ms/step - loss: 3.5234 - acc: 0.1419\n","Epoch 46/300\n","63/63 [==============================] - 1s 17ms/step - loss: 3.4850 - acc: 0.1484\n","Epoch 47/300\n","63/63 [==============================] - 1s 17ms/step - loss: 3.4675 - acc: 0.1424\n","Epoch 48/300\n","63/63 [==============================] - 1s 16ms/step - loss: 3.4307 - acc: 0.1489\n","Epoch 49/300\n","63/63 [==============================] - 1s 17ms/step - loss: 3.3947 - acc: 0.1564\n","Epoch 50/300\n","63/63 [==============================] - 1s 17ms/step - loss: 3.3654 - acc: 0.1604\n","Epoch 51/300\n","63/63 [==============================] - 1s 17ms/step - loss: 3.3309 - acc: 0.1619\n","Epoch 52/300\n","63/63 [==============================] - 1s 16ms/step - loss: 3.2912 - acc: 0.1754\n","Epoch 53/300\n","63/63 [==============================] - 1s 17ms/step - loss: 3.2723 - acc: 0.1839\n","Epoch 54/300\n","63/63 [==============================] - 1s 16ms/step - loss: 3.2276 - acc: 0.1759\n","Epoch 55/300\n","63/63 [==============================] - 1s 16ms/step - loss: 3.2048 - acc: 0.1844\n","Epoch 56/300\n","63/63 [==============================] - 1s 16ms/step - loss: 3.1678 - acc: 0.1899\n","Epoch 57/300\n","63/63 [==============================] - 1s 17ms/step - loss: 3.1519 - acc: 0.1969\n","Epoch 58/300\n","63/63 [==============================] - 1s 16ms/step - loss: 3.1071 - acc: 0.1959\n","Epoch 59/300\n","63/63 [==============================] - 1s 17ms/step - loss: 3.0795 - acc: 0.2039\n","Epoch 60/300\n","63/63 [==============================] - 1s 16ms/step - loss: 3.0457 - acc: 0.2074\n","Epoch 61/300\n","63/63 [==============================] - 1s 17ms/step - loss: 3.0100 - acc: 0.2234\n","Epoch 62/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.9749 - acc: 0.2184\n","Epoch 63/300\n","63/63 [==============================] - 1s 16ms/step - loss: 2.9465 - acc: 0.2289\n","Epoch 64/300\n","63/63 [==============================] - 1s 16ms/step - loss: 2.9222 - acc: 0.2314\n","Epoch 65/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.9016 - acc: 0.2364\n","Epoch 66/300\n","63/63 [==============================] - 1s 16ms/step - loss: 2.8788 - acc: 0.2359\n","Epoch 67/300\n","63/63 [==============================] - 1s 16ms/step - loss: 2.8368 - acc: 0.2494\n","Epoch 68/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.8180 - acc: 0.2449\n","Epoch 69/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.7910 - acc: 0.2564\n","Epoch 70/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.7711 - acc: 0.2504\n","Epoch 71/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.7359 - acc: 0.2604\n","Epoch 72/300\n","63/63 [==============================] - 1s 16ms/step - loss: 2.6956 - acc: 0.2724\n","Epoch 73/300\n","63/63 [==============================] - 1s 16ms/step - loss: 2.6869 - acc: 0.2589\n","Epoch 74/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.6453 - acc: 0.2759\n","Epoch 75/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.6325 - acc: 0.2839\n","Epoch 76/300\n","63/63 [==============================] - 1s 16ms/step - loss: 2.6011 - acc: 0.2954\n","Epoch 77/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.5856 - acc: 0.2839\n","Epoch 78/300\n","63/63 [==============================] - 1s 16ms/step - loss: 2.5668 - acc: 0.2894\n","Epoch 79/300\n","63/63 [==============================] - 1s 16ms/step - loss: 2.5334 - acc: 0.2894\n","Epoch 80/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.5210 - acc: 0.2959\n","Epoch 81/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.5093 - acc: 0.2964\n","Epoch 82/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.4900 - acc: 0.3073\n","Epoch 83/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.4540 - acc: 0.3158\n","Epoch 84/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.4380 - acc: 0.3188\n","Epoch 85/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.4229 - acc: 0.3183\n","Epoch 86/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.3892 - acc: 0.3348\n","Epoch 87/300\n","63/63 [==============================] - 1s 16ms/step - loss: 2.3741 - acc: 0.3328\n","Epoch 88/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.3426 - acc: 0.3298\n","Epoch 89/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.3268 - acc: 0.3468\n","Epoch 90/300\n","63/63 [==============================] - 1s 22ms/step - loss: 2.3122 - acc: 0.3393\n","Epoch 91/300\n","63/63 [==============================] - 2s 27ms/step - loss: 2.2920 - acc: 0.3588\n","Epoch 92/300\n","63/63 [==============================] - 1s 20ms/step - loss: 2.2762 - acc: 0.3483\n","Epoch 93/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.2492 - acc: 0.3608\n","Epoch 94/300\n","63/63 [==============================] - 1s 16ms/step - loss: 2.2401 - acc: 0.3553\n","Epoch 95/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.2232 - acc: 0.3628\n","Epoch 96/300\n","63/63 [==============================] - 1s 16ms/step - loss: 2.1911 - acc: 0.3683\n","Epoch 97/300\n","63/63 [==============================] - 1s 16ms/step - loss: 2.1757 - acc: 0.3733\n","Epoch 98/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.1660 - acc: 0.3918\n","Epoch 99/300\n","63/63 [==============================] - 1s 16ms/step - loss: 2.1484 - acc: 0.3813\n","Epoch 100/300\n","63/63 [==============================] - 1s 16ms/step - loss: 2.1250 - acc: 0.3888\n","Epoch 101/300\n","63/63 [==============================] - 1s 16ms/step - loss: 2.1021 - acc: 0.3803\n","Epoch 102/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.0953 - acc: 0.3883\n","Epoch 103/300\n","63/63 [==============================] - 1s 16ms/step - loss: 2.0723 - acc: 0.3988\n","Epoch 104/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.0555 - acc: 0.3993\n","Epoch 105/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.0426 - acc: 0.3988\n","Epoch 106/300\n","63/63 [==============================] - 1s 16ms/step - loss: 2.0277 - acc: 0.4138\n","Epoch 107/300\n","63/63 [==============================] - 1s 17ms/step - loss: 2.0088 - acc: 0.4073\n","Epoch 108/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.9907 - acc: 0.4148\n","Epoch 109/300\n","63/63 [==============================] - 1s 16ms/step - loss: 1.9883 - acc: 0.4143\n","Epoch 110/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.9733 - acc: 0.4123\n","Epoch 111/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.9660 - acc: 0.4188\n","Epoch 112/300\n","63/63 [==============================] - 1s 16ms/step - loss: 1.9303 - acc: 0.4203\n","Epoch 113/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.9150 - acc: 0.4243\n","Epoch 114/300\n","63/63 [==============================] - 1s 16ms/step - loss: 1.9111 - acc: 0.4333\n","Epoch 115/300\n","63/63 [==============================] - 1s 16ms/step - loss: 1.9009 - acc: 0.4368\n","Epoch 116/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.8770 - acc: 0.4428\n","Epoch 117/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.8703 - acc: 0.4438\n","Epoch 118/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.8578 - acc: 0.4443\n","Epoch 119/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.8385 - acc: 0.4533\n","Epoch 120/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.8378 - acc: 0.4358\n","Epoch 121/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.8096 - acc: 0.4478\n","Epoch 122/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.7984 - acc: 0.4513\n","Epoch 123/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.8030 - acc: 0.4513\n","Epoch 124/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.7845 - acc: 0.4573\n","Epoch 125/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.7774 - acc: 0.4558\n","Epoch 126/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.7507 - acc: 0.4648\n","Epoch 127/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.7445 - acc: 0.4723\n","Epoch 128/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.7512 - acc: 0.4608\n","Epoch 129/300\n","63/63 [==============================] - 1s 16ms/step - loss: 1.7289 - acc: 0.4768\n","Epoch 130/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.7026 - acc: 0.4693\n","Epoch 131/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.7060 - acc: 0.4728\n","Epoch 132/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.6869 - acc: 0.4818\n","Epoch 133/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.6760 - acc: 0.4898\n","Epoch 134/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.6739 - acc: 0.4928\n","Epoch 135/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.6657 - acc: 0.4843\n","Epoch 136/300\n","63/63 [==============================] - 1s 16ms/step - loss: 1.6561 - acc: 0.4898\n","Epoch 137/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.6433 - acc: 0.4928\n","Epoch 138/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.6362 - acc: 0.4863\n","Epoch 139/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.6311 - acc: 0.4858\n","Epoch 140/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.6213 - acc: 0.4953\n","Epoch 141/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.6136 - acc: 0.4868\n","Epoch 142/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.5805 - acc: 0.5082\n","Epoch 143/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.5808 - acc: 0.4953\n","Epoch 144/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.5701 - acc: 0.5017\n","Epoch 145/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.5609 - acc: 0.5067\n","Epoch 146/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.5711 - acc: 0.5032\n","Epoch 147/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.5353 - acc: 0.5077\n","Epoch 148/300\n","63/63 [==============================] - 1s 16ms/step - loss: 1.5458 - acc: 0.5067\n","Epoch 149/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.5318 - acc: 0.5082\n","Epoch 150/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.5347 - acc: 0.5112\n","Epoch 151/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.5294 - acc: 0.5182\n","Epoch 152/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.5142 - acc: 0.5167\n","Epoch 153/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.5033 - acc: 0.5097\n","Epoch 154/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.4884 - acc: 0.5207\n","Epoch 155/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.4892 - acc: 0.5222\n","Epoch 156/300\n","63/63 [==============================] - 1s 16ms/step - loss: 1.4815 - acc: 0.5212\n","Epoch 157/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.4802 - acc: 0.5197\n","Epoch 158/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.4677 - acc: 0.5297\n","Epoch 159/300\n","63/63 [==============================] - 1s 16ms/step - loss: 1.4723 - acc: 0.5247\n","Epoch 160/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.4448 - acc: 0.5292\n","Epoch 161/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.4417 - acc: 0.5337\n","Epoch 162/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.4349 - acc: 0.5312\n","Epoch 163/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.4476 - acc: 0.5337\n","Epoch 164/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.4167 - acc: 0.5422\n","Epoch 165/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.4200 - acc: 0.5392\n","Epoch 166/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.4205 - acc: 0.5397\n","Epoch 167/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.4026 - acc: 0.5372\n","Epoch 168/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.4008 - acc: 0.5392\n","Epoch 169/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.3866 - acc: 0.5442\n","Epoch 170/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.3906 - acc: 0.5502\n","Epoch 171/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.3887 - acc: 0.5392\n","Epoch 172/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.3854 - acc: 0.5387\n","Epoch 173/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.3776 - acc: 0.5522\n","Epoch 174/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.3681 - acc: 0.5452\n","Epoch 175/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.3565 - acc: 0.5447\n","Epoch 176/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.3516 - acc: 0.5502\n","Epoch 177/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.3347 - acc: 0.5492\n","Epoch 178/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.3357 - acc: 0.5527\n","Epoch 179/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.3319 - acc: 0.5387\n","Epoch 180/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.3231 - acc: 0.5522\n","Epoch 181/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.3278 - acc: 0.5517\n","Epoch 182/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.3054 - acc: 0.5662\n","Epoch 183/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.3142 - acc: 0.5637\n","Epoch 184/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.2965 - acc: 0.5577\n","Epoch 185/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.3079 - acc: 0.5562\n","Epoch 186/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.2872 - acc: 0.5632\n","Epoch 187/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.2718 - acc: 0.5672\n","Epoch 188/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.2944 - acc: 0.5637\n","Epoch 189/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.2833 - acc: 0.5707\n","Epoch 190/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.2930 - acc: 0.5587\n","Epoch 191/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.2762 - acc: 0.5582\n","Epoch 192/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.2656 - acc: 0.5702\n","Epoch 193/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.2696 - acc: 0.5632\n","Epoch 194/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.2567 - acc: 0.5687\n","Epoch 195/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.2578 - acc: 0.5687\n","Epoch 196/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.2513 - acc: 0.5772\n","Epoch 197/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.2439 - acc: 0.5647\n","Epoch 198/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.2458 - acc: 0.5757\n","Epoch 199/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.2314 - acc: 0.5747\n","Epoch 200/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.2568 - acc: 0.5762\n","Epoch 201/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.2270 - acc: 0.5717\n","Epoch 202/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.2192 - acc: 0.5837\n","Epoch 203/300\n","63/63 [==============================] - 2s 24ms/step - loss: 1.2142 - acc: 0.5822\n","Epoch 204/300\n","63/63 [==============================] - 2s 28ms/step - loss: 1.2212 - acc: 0.5777\n","Epoch 205/300\n","63/63 [==============================] - 1s 20ms/step - loss: 1.2142 - acc: 0.5827\n","Epoch 206/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.2072 - acc: 0.5827\n","Epoch 207/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.2035 - acc: 0.5872\n","Epoch 208/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1902 - acc: 0.5947\n","Epoch 209/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.2002 - acc: 0.5867\n","Epoch 210/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1841 - acc: 0.5957\n","Epoch 211/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1899 - acc: 0.5817\n","Epoch 212/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1957 - acc: 0.5892\n","Epoch 213/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1797 - acc: 0.5942\n","Epoch 214/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1801 - acc: 0.5897\n","Epoch 215/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1766 - acc: 0.5897\n","Epoch 216/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1622 - acc: 0.5907\n","Epoch 217/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1601 - acc: 0.5957\n","Epoch 218/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1676 - acc: 0.5967\n","Epoch 219/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1559 - acc: 0.5992\n","Epoch 220/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1594 - acc: 0.5937\n","Epoch 221/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1507 - acc: 0.5907\n","Epoch 222/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1616 - acc: 0.5967\n","Epoch 223/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1564 - acc: 0.5892\n","Epoch 224/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1475 - acc: 0.5907\n","Epoch 225/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1413 - acc: 0.5957\n","Epoch 226/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1299 - acc: 0.5922\n","Epoch 227/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.1335 - acc: 0.6012\n","Epoch 228/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1352 - acc: 0.6007\n","Epoch 229/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.1223 - acc: 0.6057\n","Epoch 230/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1227 - acc: 0.5987\n","Epoch 231/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1042 - acc: 0.6112\n","Epoch 232/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.1103 - acc: 0.5972\n","Epoch 233/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.1134 - acc: 0.5992\n","Epoch 234/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.1242 - acc: 0.6007\n","Epoch 235/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1127 - acc: 0.6062\n","Epoch 236/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.1159 - acc: 0.6042\n","Epoch 237/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.1061 - acc: 0.6052\n","Epoch 238/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0928 - acc: 0.5982\n","Epoch 239/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.1042 - acc: 0.6037\n","Epoch 240/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.1046 - acc: 0.6027\n","Epoch 241/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.1081 - acc: 0.6032\n","Epoch 242/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0845 - acc: 0.6092\n","Epoch 243/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0893 - acc: 0.6057\n","Epoch 244/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.0930 - acc: 0.6072\n","Epoch 245/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.0926 - acc: 0.6072\n","Epoch 246/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.0805 - acc: 0.6122\n","Epoch 247/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.0689 - acc: 0.6087\n","Epoch 248/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0641 - acc: 0.6087\n","Epoch 249/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.0744 - acc: 0.6037\n","Epoch 250/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.0768 - acc: 0.6137\n","Epoch 251/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0501 - acc: 0.6142\n","Epoch 252/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0643 - acc: 0.6062\n","Epoch 253/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0525 - acc: 0.6157\n","Epoch 254/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0550 - acc: 0.6132\n","Epoch 255/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0666 - acc: 0.6082\n","Epoch 256/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0725 - acc: 0.6087\n","Epoch 257/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0642 - acc: 0.6137\n","Epoch 258/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0533 - acc: 0.6132\n","Epoch 259/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0416 - acc: 0.6172\n","Epoch 260/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0364 - acc: 0.6237\n","Epoch 261/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.0304 - acc: 0.6162\n","Epoch 262/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0379 - acc: 0.6207\n","Epoch 263/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0370 - acc: 0.6227\n","Epoch 264/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0364 - acc: 0.6167\n","Epoch 265/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0417 - acc: 0.6127\n","Epoch 266/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0377 - acc: 0.6172\n","Epoch 267/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.0296 - acc: 0.6162\n","Epoch 268/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.0231 - acc: 0.6112\n","Epoch 269/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0107 - acc: 0.6292\n","Epoch 270/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.0271 - acc: 0.6147\n","Epoch 271/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.0191 - acc: 0.6207\n","Epoch 272/300\n","63/63 [==============================] - 1s 18ms/step - loss: 1.0036 - acc: 0.6332\n","Epoch 273/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0080 - acc: 0.6162\n","Epoch 274/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0093 - acc: 0.6292\n","Epoch 275/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0063 - acc: 0.6252\n","Epoch 276/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0125 - acc: 0.6257\n","Epoch 277/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0073 - acc: 0.6267\n","Epoch 278/300\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0051 - acc: 0.6202\n","Epoch 279/300\n","63/63 [==============================] - 1s 17ms/step - loss: 0.9980 - acc: 0.6342\n","Epoch 280/300\n","63/63 [==============================] - 1s 17ms/step - loss: 0.9899 - acc: 0.6287\n","Epoch 281/300\n","63/63 [==============================] - 1s 17ms/step - loss: 0.9886 - acc: 0.6267\n","Epoch 282/300\n","63/63 [==============================] - 1s 17ms/step - loss: 0.9951 - acc: 0.6307\n","Epoch 283/300\n","63/63 [==============================] - 1s 17ms/step - loss: 0.9940 - acc: 0.6207\n","Epoch 284/300\n","63/63 [==============================] - 1s 17ms/step - loss: 0.9858 - acc: 0.6277\n","Epoch 285/300\n","63/63 [==============================] - 1s 17ms/step - loss: 0.9917 - acc: 0.6282\n","Epoch 286/300\n","63/63 [==============================] - 1s 17ms/step - loss: 0.9725 - acc: 0.6412\n","Epoch 287/300\n","63/63 [==============================] - 1s 17ms/step - loss: 0.9714 - acc: 0.6402\n","Epoch 288/300\n","63/63 [==============================] - 1s 17ms/step - loss: 0.9808 - acc: 0.6342\n","Epoch 289/300\n","63/63 [==============================] - 1s 17ms/step - loss: 0.9914 - acc: 0.6302\n","Epoch 290/300\n","63/63 [==============================] - 1s 18ms/step - loss: 0.9805 - acc: 0.6317\n","Epoch 291/300\n","63/63 [==============================] - 1s 17ms/step - loss: 0.9722 - acc: 0.6292\n","Epoch 292/300\n","63/63 [==============================] - 1s 17ms/step - loss: 0.9709 - acc: 0.6307\n","Epoch 293/300\n","63/63 [==============================] - 1s 17ms/step - loss: 0.9865 - acc: 0.6357\n","Epoch 294/300\n","63/63 [==============================] - 1s 17ms/step - loss: 0.9751 - acc: 0.6312\n","Epoch 295/300\n","63/63 [==============================] - 1s 17ms/step - loss: 0.9639 - acc: 0.6397\n","Epoch 296/300\n","63/63 [==============================] - 1s 17ms/step - loss: 0.9679 - acc: 0.6217\n","Epoch 297/300\n","63/63 [==============================] - 1s 17ms/step - loss: 0.9627 - acc: 0.6367\n","Epoch 298/300\n","63/63 [==============================] - 1s 17ms/step - loss: 0.9727 - acc: 0.6327\n","Epoch 299/300\n","63/63 [==============================] - 1s 17ms/step - loss: 0.9656 - acc: 0.6417\n","Epoch 300/300\n","63/63 [==============================] - 1s 17ms/step - loss: 0.9729 - acc: 0.6302\n"]}],"source":["# 9. 순위모델 훈련\n","ratings = model_rank.fit([tf.keras.preprocessing.sequence.pad_sequences(tmp_train_r['pd_name_d'])+1e-10,\n","           tf.keras.preprocessing.sequence.pad_sequences(tmp_train_r['like'])+1e-10,\n","           tf.keras.preprocessing.sequence.pad_sequences(tmp_train_r['dislike'])+1e-10,\n","            tf.keras.preprocessing.sequence.pad_sequences(tmp_train_r['all_aisles'])+1e-10,\n","            tf.keras.preprocessing.sequence.pad_sequences(tmp_train_r['gender'])+1e-10,\n","            tf.keras.preprocessing.sequence.pad_sequences(tmp_train_r['region'])+1e-10,\n","            tf.keras.preprocessing.sequence.pad_sequences(tmp_train_r['ages'])+1e-10,\n","           ],tmp_train_r['predict_labels'].values, epochs=300)"]},{"cell_type":"code","source":["# 9. 모델 저장\n","model_rank.save(data_dir+'output/ranking.h5')"],"metadata":{"id":"0gJ9NAmzMf3m"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ylhHfFAwKte9"},"outputs":[],"source":["# 10. 순위 반영 상품 추출\n","pred = model_rank.predict([tf.keras.preprocessing.sequence.pad_sequences(tmp_test_r['pd_name_d'])+ 1e-10,\n","           tf.keras.preprocessing.sequence.pad_sequences(tmp_test_r['like'])+ 1e-10,\n","           tf.keras.preprocessing.sequence.pad_sequences(tmp_test_r['dislike'])+ 1e-10,\n","            tf.keras.preprocessing.sequence.pad_sequences(tmp_test_r['all_aisles'])+ 1e-10,\n","            tf.keras.preprocessing.sequence.pad_sequences(tmp_test_r['gender'])+ 1e-10,\n","            tf.keras.preprocessing.sequence.pad_sequences(tmp_test_r['region'])+ 1e-10,\n","            tf.keras.preprocessing.sequence.pad_sequences(tmp_test_r['ages'])+ 1e-10,\n","           ])\n","\n","N = 40 # 인당 20개의 추천\n","ranking = (-pred).argsort()[:, :N]\n","ranking[ranking>data_train['product'].max()]=0 \n","ranking_probability = np.sort(pred[:, :N])"]},{"cell_type":"markdown","metadata":{"id":"b6K3QBH2RO5C"},"source":["# 5. 평가지표값 확인"]},{"cell_type":"markdown","source":["* 생성한 모델의 평가지표값(dcg, ndcg, idcg)을 계산하는 함수를 구현합니다."],"metadata":{"id":"7fvF_NM6jkyR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kODFkOeBQPJm"},"outputs":[],"source":["# 1. 라벨값과 라벨값에 해당하는 추천 아이템 데이터 (train, test)\n","train_df = tmp_train_r[['like', 'predict_labels']]\n","train_df = train_df.rename(columns = {'like':'like_id', 'predict_labels': 'label'})\n","\n","true_df = tmp_test_r[['user', 'like', 'predict_labels']]\n","true_df = true_df.rename(columns={'like': 'true_like_id','predict_labels': 'true_labels'})\n","true_df = true_df[['user', 'true_like_id']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iaecOCP4QnTf"},"outputs":[],"source":["# 2. 고객당 추천 상품 데이터 추출\n","def user_rec(n , rank, rec_df, train_df):\n","   # n: 고객 순서\n","   # rank: 순위모델 결과\n","   # rec_df: 고객당 추천된 라벨과 라벨에 해당하는 상품 아이디\n","   # train_df: tmp_train_r에서 필요한 like, predict_labels만 추출한 데이터프레임\n","  for label in rank[n-1]: # 순위 모델 결과의 라벨들에 대해\n","    rec_id = train_df['like_id'][(train_df['label']==label)].tolist() # 각 라벨에 해당하는 상품 id 추출\n","    rec_df = rec_df.append(pd.DataFrame([[label, rec_id]], columns=['label', 'rec_id']), ignore_index=True) # 해당 고객을 위한 추천 상품 정리\n","  return rec_df\n","\n","for i in range(len(tmp_test_r)):\n","    user_rec_data = pd.DataFrame(columns=['label', 'rec_id'])\n","    globals()['rec_df'+str(i)] = user_rec(i, ranking, user_rec_data, train_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SsvqGf-3QukR"},"outputs":[],"source":["# 2. 각 유저별 추천목록의 상품들 중 실제로 재구매한(좋아한) 것이 있는지 확인\n","def true_like(n, rec_df, true_df):\n","   # n: 고객 순서\n","   # rec_df: 고객당 추천된 라벨과 라벨에 해당하는 상품 아이디\n","   # true_df: tmp_test_r에서 필요한 user_id, true_like_id만 추출한 데이터프레임\n","  tf_list = []\n","  rec_id = rec_df['rec_id'].tolist() \n","  true = true_df['true_like_id'].tolist()[n-1] # true데이터인 test_data의 형태상, row에 있는 유저 n-1번째 row를 고정해야 함. \n","\n","  for rec in rec_id[:][:]:\n","    if len(rec) == 0: tf_list.append(0.0)\n","    elif len(rec)>=1:        \n","      tf = true[0] in rec[0]\n","      if tf == True: tf_list.append(1)\n","      else: tf_list.append(0)\n","      rec_df['T/F'] = pd.DataFrame(tf_list)\n","  return rec_df\n","\n","for i in range(len(tmp_test_r)):\n","    globals()['rec_df'+str(i)] = true_like(i, globals()['rec_df'+str(i)], true_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i58lQLPVQxTA"},"outputs":[],"source":["# 3. 각 유저별 dcg, idcg, ndcg 계산\n","def get_ndcg(rec_df):\n","  # rec_df: 유저당 추천된 '라벨'과 '라벨에 해당하는 상품 아이디', 'T/F(true_like여부)'특성이 포함된 데이터프레임 \n","  rec = rec_df['rec_id'].tolist()\n","  t = rec_df['T/F'].tolist()\n","  dcg = 0.0\n","  \n","  # dcg 계산 - 해당 모델의 추천 순위 성능\n","  # idcg 계산 - 가장 이상적인 모델의 추천순위 성능\n","  # ndcg 계산 - 비율\n","  for i, j in enumerate(t): \n","    if j == 1.0: dcg += (1.0/np.log2(i+1+1))\n","    else: dcg += 0\n","  idcg = sum((1.0/np.log2(i+1+1) for i in range(0, len(t)+1)))\n","  ndcg = dcg / idcg \n","\n","  ndcg_df = pd.DataFrame(columns=['dcg', 'idcg', 'ndcg'])\n","  ndcg_df = ndcg_df.append(pd.DataFrame([[dcg, idcg, ndcg]], columns=['dcg', 'idcg', 'ndcg']), ignore_index=True)\n","  return ndcg_df\n","\n","for i in range(len(tmp_test_r)):\n","    globals()['ndcg_df'+str(i)]  = get_ndcg(globals()['rec_df'+str(i)])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xxNGFymlQzb_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660232279286,"user_tz":-540,"elapsed":4,"user":{"displayName":"안도현","userId":"08395434251954901727"}},"outputId":"c1a4afb8-c235-4bba-9f9b-904d2907ea83"},"outputs":[{"output_type":"stream","name":"stdout","text":["[ ranking model의 dcg, idcg, ndcg평균 ] \n"," dcg      2.220527\n","idcg    11.276482\n","ndcg     0.196917\n","dtype: float64\n"]}],"source":["# 4. user별 dcg, idcg, ndcg 결과 취합\n","def concat_result(df1, df2):\n","  df3 = pd.concat([df1, df2])\n","  return df3\n","\n","result = concat_result(ndcg_df0, ndcg_df1)\n","for i in range(2, len(tmp_test_r)):\n","  result = concat_result(result, globals()['ndcg_df'+str(i)])\n","\n","print('[ ranking model의 dcg, idcg, ndcg평균 ] \\n', result.mean())"]},{"cell_type":"code","source":[""],"metadata":{"id":"G20wcULqd1do"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"DNN.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}